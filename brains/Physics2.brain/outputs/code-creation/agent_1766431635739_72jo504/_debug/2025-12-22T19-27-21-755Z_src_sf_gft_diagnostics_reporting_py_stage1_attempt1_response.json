{
  "content": "Implemented `src/sf_gft_diagnostics/reporting.py` with lightweight, reproducible report generation that standardizes inputs into ranked summaries and writes JSON/CSV outputs plus optional matplotlib plots for top-priority observables and scaling-fit values.",
  "reasoning": "",
  "responseId": "resp_09bd9463a36544920069499da7b0888197885463265baa6025",
  "model": "gpt-5.2-2025-12-11",
  "usage": {
    "input_tokens": 4862,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 3237,
    "output_tokens_details": {
      "reasoning_tokens": 297
    },
    "total_tokens": 8099
  },
  "hadError": false,
  "errorType": null,
  "webSearchSources": [],
  "citations": [],
  "output": [
    {
      "id": "rs_09bd9463a36544920069499da86ee48197be79b760fd692940",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "ci_09bd9463a36544920069499daffd988197b591d59b641c68e0",
      "type": "code_interpreter_call",
      "status": "completed",
      "code": "from pathlib import Path\nimport json, textwrap, os, csv, math, statistics\ntarget_path = Path('/mnt/data').joinpath('src/sf_gft_diagnostics/reporting.py')\ntarget_path.parent.mkdir(parents=True, exist_ok=True)\n\nchunks = []\nchunks.append(\"\"\"\n\\\"\\\"\\\"Reporting utilities for spin-foam/GFT continuum-recovery diagnostics.\n\nThis module focuses on *reproducible* outputs: JSON bundles + CSV tables and\n(optional) simple plots. It is intentionally lightweight and can consume\noutputs produced by other parts of the package (observables/scaling/metrics).\n\\\"\\\"\\\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, Iterable, List, Mapping, Optional, Sequence, Tuple\nimport csv\nimport json\nimport math\nimport statistics\n\ntry:  # optional\n    import numpy as _np  # type: ignore\nexcept Exception:  # pragma: no cover\n    _np = None\n\ntry:  # optional\n    import matplotlib.pyplot as _plt  # type: ignore\nexcept Exception:  # pragma: no cover\n    _plt = None\n\n\ndef _as_records(x: Any) -> List[Dict[str, Any]]:\n    \\\"\\\"\\\"Normalize common container types into a list of dict records.\\\"\\\"\\\"\n    if x is None:\n        return []\n    if isinstance(x, list):\n        return [dict(r) for r in x]\n    if isinstance(x, dict):\n        # dict-of-dicts or dict-of-scalars\n        if all(isinstance(v, Mapping) for v in x.values()):\n            return [dict(v, key=k) for k, v in x.items()]  # type: ignore[arg-type]\n        return [dict(x)]\n    raise TypeError(f\\\"Unsupported record container: {type(x)!r}\\\")\n\n\ndef _write_json(path: Path, obj: Any) -> None:\n    path.write_text(json.dumps(obj, indent=2, sort_keys=True), encoding=\\\"utf-8\\\")\n\n\ndef _write_csv(path: Path, records: Sequence[Mapping[str, Any]]) -> None:\n    records = list(records)\n    if not records:\n        path.write_text(\\\"\\\", encoding=\\\"utf-8\\\")\n        return\n    keys: List[str] = sorted({k for r in records for k in r.keys()})\n    with path.open(\\\"w\\\", newline=\\\"\\\", encoding=\\\"utf-8\\\") as f:\n        w = csv.DictWriter(f, fieldnames=keys)\n        w.writeheader()\n        for r in records:\n            w.writerow({k: r.get(k, \\\"\\\") for k in keys})\n\n\ndef _rank(records: List[Dict[str, Any]], key: str, reverse: bool = True) -> List[Dict[str, Any]]:\n    def _val(r: Mapping[str, Any]) -> float:\n        v = r.get(key, float(\\\"nan\\\"))\n        try:\n            return float(v)\n        except Exception:\n            return float(\\\"nan\\\")\n    ranked = sorted(records, key=_val, reverse=reverse)\n    for i, r in enumerate(ranked, start=1):\n        r[\\\"rank\\\"] = i\n    return ranked\n\"\"\")\n\nchunks.append(\"\"\"\ndef summarize_priorities(observables: Any) -> Dict[str, Any]:\n    \\\"\\\"\\\"Summarize/standardize a prioritized observable catalog.\n\n    Expected record fields (best-effort): name, priority, family, estimator, notes.\n    \\\"\\\"\\\"\n    recs = _as_records(observables)\n    for r in recs:\n        r.setdefault(\\\"priority\\\", r.get(\\\"score\\\", 0.0))\n        r.setdefault(\\\"name\\\", r.get(\\\"id\\\", r.get(\\\"label\\\", \\\"\\\")))\n    ranked = _rank(recs, key=\\\"priority\\\", reverse=True)\n    top_families: Dict[str, int] = {}\n    for r in ranked:\n        fam = str(r.get(\\\"family\\\", \\\"unknown\\\"))\n        top_families[fam] = top_families.get(fam, 0) + 1\n    return {\\\"n\\\": len(ranked), \\\"top_families\\\": top_families, \\\"records\\\": ranked}\n\n\ndef summarize_scaling_fits(fits: Any) -> Dict[str, Any]:\n    \\\"\\\"\\\"Summarize scaling-fit outputs.\n\n    Expected fields per fit: observable/name, exponent/nu/eta, stderr, r2, chi2_red.\n    \\\"\\\"\\\"\n    recs = _as_records(fits)\n    for r in recs:\n        r.setdefault(\\\"name\\\", r.get(\\\"observable\\\", r.get(\\\"id\\\", \\\"\\\")))\n        for k in (\\\"exponent\\\", \\\"nu\\\", \\\"eta\\\"):\n            if k in r and \\\"value\\\" not in r:\n                r[\\\"value\\\"] = r[k]\n        r.setdefault(\\\"stderr\\\", r.get(\\\"se\\\", r.get(\\\"sigma\\\", None)))\n        # heuristic fit-quality score: prefer high r2, low chi2\n        r2 = float(r.get(\\\"r2\\\", 0.0) or 0.0)\n        chi = float(r.get(\\\"chi2_red\\\", r.get(\\\"chi2\\\", 1.0)) or 1.0)\n        r[\\\"fit_score\\\"] = r2 / max(chi, 1e-12)\n    ranked = _rank(recs, key=\\\"fit_score\\\", reverse=True)\n    values = [float(r.get(\\\"value\\\")) for r in ranked if _is_finite(r.get(\\\"value\\\"))]\n    return {\n        \\\"n\\\": len(ranked),\n        \\\"value_mean\\\": statistics.fmean(values) if values else None,\n        \\\"value_stdev\\\": statistics.pstdev(values) if len(values) > 1 else None,\n        \\\"records\\\": ranked,\n    }\n\n\ndef _is_finite(x: Any) -> bool:\n    try:\n        return math.isfinite(float(x))\n    except Exception:\n        return False\n\n\ndef summarize_cv(scores: Any) -> Dict[str, Any]:\n    \\\"\\\"\\\"Summarize cross-validation scores across diagnostics/models.\n\n    Records should include: name/model, fold scores or mean/std already computed.\n    \\\"\\\"\\\"\n    recs = _as_records(scores)\n    for r in recs:\n        r.setdefault(\\\"name\\\", r.get(\\\"model\\\", r.get(\\\"observable\\\", r.get(\\\"id\\\", \\\"\\\"))))\n        if \\\"folds\\\" in r and isinstance(r[\\\"folds\\\"], Sequence):\n            vals = [float(v) for v in r[\\\"folds\\\"] if _is_finite(v)]\n            r[\\\"cv_mean\\\"] = statistics.fmean(vals) if vals else None\n            r[\\\"cv_std\\\"] = statistics.pstdev(vals) if len(vals) > 1 else 0.0\n        r.setdefault(\\\"cv_mean\\\", r.get(\\\"score\\\", None))\n    ranked = _rank(recs, key=\\\"cv_mean\\\", reverse=True)\n    return {\\\"n\\\": len(ranked), \\\"records\\\": ranked}\n\n\ndef summarize_benchmarks(benchmarks: Any) -> Dict[str, Any]:\n    \\\"\\\"\\\"Summarize benchmark comparisons (semiclassical, TN/lattice RG, etc.).\n\n    Records should include: name, metric, value, reference, and optionally tolerance.\n    A normalized \\\"z\\\" score is computed when possible.\n    \\\"\\\"\\\"\n    recs = _as_records(benchmarks)\n    for r in recs:\n        r.setdefault(\\\"name\\\", r.get(\\\"observable\\\", r.get(\\\"id\\\", \\\"\\\")))\n        val, ref = r.get(\\\"value\\\"), r.get(\\\"reference\\\")\n        tol = r.get(\\\"tolerance\\\", r.get(\\\"stderr\\\", r.get(\\\"sigma\\\", None)))\n        if _is_finite(val) and _is_finite(ref) and _is_finite(tol) and float(tol) > 0:\n            r[\\\"z\\\"] = (float(val) - float(ref)) / float(tol)\n            r[\\\"within_tol\\\"] = abs(float(r[\\\"z\\\"])) <= 1.0\n        elif _is_finite(val) and _is_finite(ref):\n            r[\\\"abs_err\\\"] = abs(float(val) - float(ref))\n    # prefer small |z|, else small abs_err\n    for r in recs:\n        z = r.get(\\\"z\\\", None)\n        r[\\\"bench_score\\\"] = -abs(float(z)) if _is_finite(z) else -float(r.get(\\\"abs_err\\\", float(\\\"inf\\\")))\n    ranked = _rank(recs, key=\\\"bench_score\\\", reverse=True)\n    return {\\\"n\\\": len(ranked), \\\"records\\\": ranked}\n\"\"\")\n\nchunks.append(\"\"\"\n@dataclass(frozen=True)\nclass ReportPaths:\n    out_dir: Path\n    prefix: str = \\\"sf_gft\\\"\n\n    def file(self, stem: str, ext: str) -> Path:\n        return self.out_dir / f\\\"{self.prefix}_{stem}.{ext.lstrip('.')}\\\"\n\n\ndef generate_report_bundle(\n    out_dir: str | Path,\n    *,\n    prefix: str = \\\"sf_gft\\\",\n    observables: Any = None,\n    scaling_fits: Any = None,\n    cv_scores: Any = None,\n    benchmarks: Any = None,\n    make_plots: bool = True,\n) -> Dict[str, Any]:\n    \\\"\\\"\\\"Create a compact report bundle (JSON + CSV + optional plots).\n\n    Returns the in-memory summary dict that is also written to `<prefix>_report.json`.\n    \\\"\\\"\\\"\n    paths = ReportPaths(Path(out_dir), prefix=prefix)\n    paths.out_dir.mkdir(parents=True, exist_ok=True)\n\n    summary: Dict[str, Any] = {\\\"schema\\\": \\\"sf_gft_diagnostics.report.v1\\\"}\n    if observables is not None:\n        summary[\\\"observables\\\"] = summarize_priorities(observables)\n        _write_csv(paths.file(\\\"observables\\\", \\\"csv\\\"), summary[\\\"observables\\\"][\\\"records\\\"])\n    if scaling_fits is not None:\n        summary[\\\"scaling_fits\\\"] = summarize_scaling_fits(scaling_fits)\n        _write_csv(paths.file(\\\"scaling_fits\\\", \\\"csv\\\"), summary[\\\"scaling_fits\\\"][\\\"records\\\"])\n    if cv_scores is not None:\n        summary[\\\"cv_scores\\\"] = summarize_cv(cv_scores)\n        _write_csv(paths.file(\\\"cv_scores\\\", \\\"csv\\\"), summary[\\\"cv_scores\\\"][\\\"records\\\"])\n    if benchmarks is not None:\n        summary[\\\"benchmarks\\\"] = summarize_benchmarks(benchmarks)\n        _write_csv(paths.file(\\\"benchmarks\\\", \\\"csv\\\"), summary[\\\"benchmarks\\\"][\\\"records\\\"])\n\n    _write_json(paths.file(\\\"report\\\", \\\"json\\\"), summary)\n\n    if make_plots and _plt is not None:\n        _maybe_plot_top_observables(paths, summary.get(\\\"observables\\\"))\n        _maybe_plot_scaling_values(paths, summary.get(\\\"scaling_fits\\\"))\n    return summary\n\n\ndef _maybe_plot_top_observables(paths: ReportPaths, obs_summary: Optional[Mapping[str, Any]]) -> None:\n    if not obs_summary:\n        return\n    recs = list(obs_summary.get(\\\"records\\\", []))[:10]\n    names = [str(r.get(\\\"name\\\", \\\"\\\")) for r in recs]\n    vals = [float(r.get(\\\"priority\\\", 0.0) or 0.0) for r in recs]\n    fig = _plt.figure(figsize=(8, 3.2))\n    ax = fig.add_subplot(111)\n    ax.bar(range(len(vals)), vals)\n    ax.set_xticks(range(len(vals)))\n    ax.set_xticklabels(names, rotation=45, ha=\\\"right\\\", fontsize=8)\n    ax.set_ylabel(\\\"priority\\\")\n    ax.set_title(\\\"Top continuum diagnostics (priority)\\\")\n    fig.tight_layout()\n    fig.savefig(paths.file(\\\"observables_top\\\", \\\"png\\\"), dpi=160)\n    _plt.close(fig)\n\n\ndef _maybe_plot_scaling_values(paths: ReportPaths, fit_summary: Optional[Mapping[str, Any]]) -> None:\n    if not fit_summary:\n        return\n    recs = [r for r in fit_summary.get(\\\"records\\\", []) if _is_finite(r.get(\\\"value\\\"))][:15]\n    if not recs:\n        return\n    names = [str(r.get(\\\"name\\\", \\\"\\\")) for r in recs]\n    vals = [float(r.get(\\\"value\\\")) for r in recs]\n    err = [float(r.get(\\\"stderr\\\")) if _is_finite(r.get(\\\"stderr\\\")) else 0.0 for r in recs]\n    fig = _plt.figure(figsize=(8, 3.2))\n    ax = fig.add_subplot(111)\n    ax.errorbar(range(len(vals)), vals, yerr=err, fmt=\\\"o\\\", capsize=3)\n    ax.set_xticks(range(len(vals)))\n    ax.set_xticklabels(names, rotation=45, ha=\\\"right\\\", fontsize=8)\n    ax.set_ylabel(\\\"scaling value\\\")\n    ax.set_title(\\\"Scaling-fit values (with stderr if provided)\\\")\n    fig.tight_layout()\n    fig.savefig(paths.file(\\\"scaling_values\\\", \\\"png\\\"), dpi=160)\n    _plt.close(fig)\n\"\"\")\n\nfinal_text = '\\n'.join(block.strip('\\n') for block in chunks).strip() + '\\n'\ntarget_path.write_text(final_text, encoding='utf-8')\nprint('FILE_WRITTEN:src/sf_gft_diagnostics/reporting.py')\nprint('DIR_STATE:' + json.dumps(sorted(str(p.relative_to(Path(\"/mnt/data\"))) for p in target_path.parent.glob('*') if p.is_file())))\n",
      "container_id": "cntr_69499b97284881909e58fb047f14c1e300fdec2b43878464",
      "outputs": [
        {
          "type": "logs",
          "logs": "FILE_WRITTEN:src/sf_gft_diagnostics/reporting.py\nDIR_STATE:[\"src/sf_gft_diagnostics/__init__.py\", \"src/sf_gft_diagnostics/benchmarks.py\", \"src/sf_gft_diagnostics/metrics.py\", \"src/sf_gft_diagnostics/observables.py\", \"src/sf_gft_diagnostics/reporting.py\", \"src/sf_gft_diagnostics/rg_io.py\", \"src/sf_gft_diagnostics/scaling.py\"]\n"
        }
      ]
    },
    {
      "id": "msg_09bd9463a36544920069499de16f7881978321f052321ebc49",
      "type": "message",
      "status": "completed",
      "content": [
        {
          "type": "output_text",
          "annotations": [],
          "logprobs": [],
          "text": "Implemented `src/sf_gft_diagnostics/reporting.py` with lightweight, reproducible report generation that standardizes inputs into ranked summaries and writes JSON/CSV outputs plus optional matplotlib plots for top-priority observables and scaling-fit values."
        }
      ],
      "role": "assistant"
    }
  ],
  "codeResults": []
}