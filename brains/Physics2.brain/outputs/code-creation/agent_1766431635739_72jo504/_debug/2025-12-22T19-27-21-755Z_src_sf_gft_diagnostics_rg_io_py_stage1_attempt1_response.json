{
  "content": "Implemented `rg_io.py` with compact `RGDataset`/`RGStep` dataclasses plus JSON/JSONL loaders and normalization/merge utilities to unify tensor-network/lattice RG and GFT/spinfoam coarse-graining logs into a consistent in-memory trajectory format.",
  "reasoning": "",
  "responseId": "resp_09072f9ede3124240069499ced57748195b3086dc616cd1312",
  "model": "gpt-5.2-2025-12-11",
  "usage": {
    "input_tokens": 5575,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 3941,
    "output_tokens_details": {
      "reasoning_tokens": 277
    },
    "total_tokens": 9516
  },
  "hadError": false,
  "errorType": null,
  "webSearchSources": [],
  "citations": [],
  "output": [
    {
      "id": "rs_09072f9ede3124240069499cedfb9c81958d36e57ea70a620e",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "ci_09072f9ede3124240069499cf4e7b0819586964e5bab09127f",
      "type": "code_interpreter_call",
      "status": "completed",
      "code": "from pathlib import Path\nimport json, textwrap, os, re, math\ntarget_path = Path('/mnt/data').joinpath('src/sf_gft_diagnostics/rg_io.py')\ntarget_path.parent.mkdir(parents=True, exist_ok=True)\n\nchunks = []\nchunks.append(r\"\"\"\n\"\"\"rg_io.py: lightweight loaders/adapters for RG/coarse-graining outputs.\n\nThis module normalizes heterogeneous logs (tensor-network/lattice RG, or\nGFT/spinfoam coarse-graining) into a simple in-memory dataset: a sequence of\n\"steps\" with scale information, couplings/parameters, observables, and optional\ndistributional payloads.\n\"\"\"\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Any, Dict, Iterable, List, Mapping, Optional, Sequence, Union\nimport json\n\n\nJson = Union[dict, list, str, int, float, bool, None]\n\n\n@dataclass\nclass RGStep:\n    \"\"\"One RG/coarse-graining step.\"\"\"\n    index: int\n    scale: Optional[float] = None          # e.g., lattice spacing, blocking factor, k\n    level: Optional[int] = None            # e.g., iteration / coarse-graining level\n    params: Dict[str, float] = field(default_factory=dict)\n    observables: Dict[str, Any] = field(default_factory=dict)\n    distributions: Dict[str, Any] = field(default_factory=dict)\n    meta: Dict[str, Any] = field(default_factory=dict)\n\n\n@dataclass\nclass RGDataset:\n    \"\"\"Normalized dataset holding a flow/trajectory.\"\"\"\n    name: str = \"rg_dataset\"\n    kind: str = \"auto\"                     # 'tnrg', 'lattice_rg', 'gft', 'spinfoam', ...\n    meta: Dict[str, Any] = field(default_factory=dict)\n    steps: List[RGStep] = field(default_factory=list)\n\n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"name\": self.name,\n            \"kind\": self.kind,\n            \"meta\": self.meta,\n            \"steps\": [s.__dict__ for s in self.steps],\n        }\n\"\"\")\n\nchunks.append(r\"\"\"\ndef _as_float(x: Any) -> Optional[float]:\n    try:\n        if x is None:\n            return None\n        return float(x)\n    except Exception:\n        return None\n\n\ndef _coerce_num_map(m: Mapping[str, Any]) -> Dict[str, float]:\n    out: Dict[str, float] = {}\n    for k, v in m.items():\n        fv = _as_float(v)\n        if fv is not None:\n            out[str(k)] = fv\n    return out\n\n\ndef _normalize_step(rec: Mapping[str, Any], idx: int) -> RGStep:\n    # Accept a few common synonyms across ecosystems.\n    scale = _as_float(rec.get(\"scale\", rec.get(\"k\", rec.get(\"a\", rec.get(\"lambda\")))))\n    level = rec.get(\"level\", rec.get(\"iter\", rec.get(\"iteration\", rec.get(\"n\"))))\n    try:\n        level = int(level) if level is not None else None\n    except Exception:\n        level = None\n\n    params_raw = rec.get(\"params\", rec.get(\"couplings\", rec.get(\"theta\", {}))) or {}\n    obs_raw = rec.get(\"observables\", rec.get(\"obs\", rec.get(\"measurements\", {}))) or {}\n    dist_raw = rec.get(\"distributions\", rec.get(\"dists\", rec.get(\"histograms\", {}))) or {}\n\n    meta = dict(rec.get(\"meta\", {}))\n    # Preserve unrecognized top-level keys without duplicating common fields.\n    for k, v in rec.items():\n        if k in {\"scale\",\"k\",\"a\",\"lambda\",\"level\",\"iter\",\"iteration\",\"n\",\n                 \"params\",\"couplings\",\"theta\",\"observables\",\"obs\",\"measurements\",\n                 \"distributions\",\"dists\",\"histograms\",\"meta\"}:\n            continue\n        meta.setdefault(k, v)\n\n    return RGStep(\n        index=idx,\n        scale=scale,\n        level=level,\n        params=_coerce_num_map(params_raw) if isinstance(params_raw, Mapping) else {},\n        observables=dict(obs_raw) if isinstance(obs_raw, Mapping) else {\"_value\": obs_raw},\n        distributions=dict(dist_raw) if isinstance(dist_raw, Mapping) else {\"_value\": dist_raw},\n        meta=meta,\n    )\n\"\"\")\n\nchunks.append(r\"\"\"\ndef from_records(\n    records: Sequence[Mapping[str, Any]],\n    *,\n    name: str = \"rg_dataset\",\n    kind: str = \"auto\",\n    meta: Optional[Mapping[str, Any]] = None,\n) -> RGDataset:\n    \"\"\"Build a normalized dataset from a list of dict records.\"\"\"\n    ds = RGDataset(name=name, kind=kind, meta=dict(meta or {}))\n    ds.steps = [_normalize_step(r, i) for i, r in enumerate(records)]\n    return ds\n\n\ndef _load_json_any(path: Path) -> Json:\n    txt = path.read_text(encoding=\"utf-8\")\n    return json.loads(txt)\n\n\ndef _load_jsonl(path: Path) -> List[dict]:\n    out: List[dict] = []\n    for line in path.read_text(encoding=\"utf-8\").splitlines():\n        line = line.strip()\n        if not line or line.startswith(\"#\"):\n            continue\n        out.append(json.loads(line))\n    return out\n\n\ndef load_rg_dataset(\n    path: Union[str, Path],\n    *,\n    kind: str = \"auto\",\n    name: Optional[str] = None,\n) -> RGDataset:\n    \"\"\"Load a dataset from .json or .jsonl (newline-delimited JSON).\n\n    Supported structures:\n      - list[dict]: interpreted as step records\n      - dict with key 'steps' or 'records': interpreted as a container\n      - dict with key 'flow' or 'trajectory': steps under that key\n    \"\"\"\n    p = Path(path)\n    nm = name or p.stem\n\n    if p.suffix.lower() in {\".jsonl\", \".ndjson\"}:\n        records = _load_jsonl(p)\n        return from_records(records, name=nm, kind=kind, meta={\"source\": str(p)})\n\n    obj = _load_json_any(p)\n    if isinstance(obj, list):\n        return from_records(obj, name=nm, kind=kind, meta={\"source\": str(p)})\n    if not isinstance(obj, dict):\n        raise TypeError(f\"Unsupported JSON root type: {type(obj).__name__}\")\n\n    container = obj\n    steps = None\n    for key in (\"steps\", \"records\", \"flow\", \"trajectory\"):\n        if key in container:\n            steps = container[key]\n            break\n    if steps is None and all(isinstance(v, dict) for v in container.values()):\n        # Some logs use {0:{...},1:{...}}; sort by key.\n        try:\n            items = sorted(container.items(), key=lambda kv: int(str(kv[0])))\n            steps = [v for _, v in items]\n        except Exception:\n            steps = list(container.values())\n\n    if not isinstance(steps, list):\n        raise ValueError(\"Could not locate a list of step records in JSON.\")\n\n    meta = dict(container.get(\"meta\", {})) if isinstance(container.get(\"meta\", {}), dict) else {}\n    meta.setdefault(\"source\", str(p))\n    meta.setdefault(\"container_keys\", sorted(container.keys()))\n    return from_records(steps, name=nm, kind=kind, meta=meta)\n\n\ndef merge_datasets(*datasets: RGDataset, name: str = \"merged\") -> RGDataset:\n    \"\"\"Concatenate multiple datasets (useful for multi-run ensembles).\"\"\"\n    meta = {\"merged_from\": [d.name for d in datasets]}\n    steps: List[RGStep] = []\n    for di, d in enumerate(datasets):\n        for s in d.steps:\n            s2 = RGStep(**{**s.__dict__})\n            s2.meta = dict(s2.meta)\n            s2.meta.setdefault(\"run_index\", di)\n            steps.append(s2)\n    return RGDataset(name=name, kind=\"merged\", meta=meta, steps=steps)\n\"\"\")\n\nfinal_text = '\\n'.join(block.strip('\\n') for block in chunks).strip() + '\\n'\ntarget_path.write_text(final_text, encoding='utf-8')\nprint('FILE_WRITTEN:src/sf_gft_diagnostics/rg_io.py')\nprint('DIR_STATE:' + json.dumps(sorted(str(p.relative_to(Path(\"/mnt/data\"))) for p in target_path.parent.glob('*') if p.is_file())))\n",
      "container_id": "cntr_69499b97284881909e58fb047f14c1e300fdec2b43878464",
      "outputs": []
    },
    {
      "id": "rs_09072f9ede3124240069499d16789081959849df541a101855",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "ci_09072f9ede3124240069499d17aefc819584d132d788931840",
      "type": "code_interpreter_call",
      "status": "completed",
      "code": "from pathlib import Path\nimport json\ntarget_path = Path('/mnt/data').joinpath('src/sf_gft_diagnostics/rg_io.py')\ntarget_path.parent.mkdir(parents=True, exist_ok=True)\n\nchunks = []\nchunks.append('''\\\n\"\"\"rg_io.py: lightweight loaders/adapters for RG/coarse-graining outputs.\n\nThis module normalizes heterogeneous logs (tensor-network/lattice RG, or\nGFT/spinfoam coarse-graining) into a simple in-memory dataset: a sequence of\n\"steps\" with scale information, couplings/parameters, observables, and optional\ndistributional payloads.\n\"\"\"\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Mapping, Optional, Sequence, Union\nimport json\n\n\nJson = Union[dict, list, str, int, float, bool, None]\n\n\n@dataclass\nclass RGStep:\n    \"\"\"One RG/coarse-graining step.\"\"\"\n    index: int\n    scale: Optional[float] = None          # e.g., lattice spacing, blocking factor, k\n    level: Optional[int] = None            # e.g., iteration / coarse-graining level\n    params: Dict[str, float] = field(default_factory=dict)\n    observables: Dict[str, Any] = field(default_factory=dict)\n    distributions: Dict[str, Any] = field(default_factory=dict)\n    meta: Dict[str, Any] = field(default_factory=dict)\n\n\n@dataclass\nclass RGDataset:\n    \"\"\"Normalized dataset holding a flow/trajectory.\"\"\"\n    name: str = \"rg_dataset\"\n    kind: str = \"auto\"                     # 'tnrg', 'lattice_rg', 'gft', 'spinfoam', ...\n    meta: Dict[str, Any] = field(default_factory=dict)\n    steps: List[RGStep] = field(default_factory=list)\n\n    def to_dict(self) -> Dict[str, Any]:\n        return {\"name\": self.name, \"kind\": self.kind, \"meta\": self.meta,\n                \"steps\": [s.__dict__ for s in self.steps]}\n''')\n\nchunks.append('''\\\ndef _as_float(x: Any) -> Optional[float]:\n    try:\n        if x is None:\n            return None\n        return float(x)\n    except Exception:\n        return None\n\n\ndef _coerce_num_map(m: Mapping[str, Any]) -> Dict[str, float]:\n    out: Dict[str, float] = {}\n    for k, v in m.items():\n        fv = _as_float(v)\n        if fv is not None:\n            out[str(k)] = fv\n    return out\n\n\ndef _normalize_step(rec: Mapping[str, Any], idx: int) -> RGStep:\n    # Accept a few common synonyms across ecosystems.\n    scale = _as_float(rec.get(\"scale\", rec.get(\"k\", rec.get(\"a\", rec.get(\"lambda\")))))\n    level = rec.get(\"level\", rec.get(\"iter\", rec.get(\"iteration\", rec.get(\"n\"))))\n    try:\n        level = int(level) if level is not None else None\n    except Exception:\n        level = None\n\n    params_raw = rec.get(\"params\", rec.get(\"couplings\", rec.get(\"theta\", {}))) or {}\n    obs_raw = rec.get(\"observables\", rec.get(\"obs\", rec.get(\"measurements\", {}))) or {}\n    dist_raw = rec.get(\"distributions\", rec.get(\"dists\", rec.get(\"histograms\", {}))) or {}\n\n    meta = dict(rec.get(\"meta\", {}))\n    # Preserve unrecognized top-level keys without duplicating common fields.\n    skip = {\"scale\",\"k\",\"a\",\"lambda\",\"level\",\"iter\",\"iteration\",\"n\",\n            \"params\",\"couplings\",\"theta\",\"observables\",\"obs\",\"measurements\",\n            \"distributions\",\"dists\",\"histograms\",\"meta\"}\n    for k, v in rec.items():\n        if k not in skip:\n            meta.setdefault(k, v)\n\n    return RGStep(\n        index=idx,\n        scale=scale,\n        level=level,\n        params=_coerce_num_map(params_raw) if isinstance(params_raw, Mapping) else {},\n        observables=dict(obs_raw) if isinstance(obs_raw, Mapping) else {\"_value\": obs_raw},\n        distributions=dict(dist_raw) if isinstance(dist_raw, Mapping) else {\"_value\": dist_raw},\n        meta=meta,\n    )\n''')\n\nchunks.append('''\\\ndef from_records(\n    records: Sequence[Mapping[str, Any]],\n    *,\n    name: str = \"rg_dataset\",\n    kind: str = \"auto\",\n    meta: Optional[Mapping[str, Any]] = None,\n) -> RGDataset:\n    \"\"\"Build a normalized dataset from a list of dict records.\"\"\"\n    ds = RGDataset(name=name, kind=kind, meta=dict(meta or {}))\n    ds.steps = [_normalize_step(r, i) for i, r in enumerate(records)]\n    return ds\n\n\ndef _load_json_any(path: Path) -> Json:\n    return json.loads(path.read_text(encoding=\"utf-8\"))\n\n\ndef _load_jsonl(path: Path) -> List[dict]:\n    out: List[dict] = []\n    for line in path.read_text(encoding=\"utf-8\").splitlines():\n        line = line.strip()\n        if not line or line.startswith(\"#\"):\n            continue\n        out.append(json.loads(line))\n    return out\n\n\ndef load_rg_dataset(\n    path: Union[str, Path],\n    *,\n    kind: str = \"auto\",\n    name: Optional[str] = None,\n) -> RGDataset:\n    \"\"\"Load a dataset from .json or .jsonl (newline-delimited JSON).\n\n    Accepted structures:\n      - list[dict]: step records\n      - dict with key 'steps'/'records'/'flow'/'trajectory': container\n      - dict-of-dicts: keys sortable to steps (e.g., {0:{...},1:{...}})\n    \"\"\"\n    p = Path(path)\n    nm = name or p.stem\n\n    if p.suffix.lower() in {\".jsonl\", \".ndjson\"}:\n        return from_records(_load_jsonl(p), name=nm, kind=kind, meta={\"source\": str(p)})\n\n    obj = _load_json_any(p)\n    if isinstance(obj, list):\n        return from_records(obj, name=nm, kind=kind, meta={\"source\": str(p)})\n    if not isinstance(obj, dict):\n        raise TypeError(f\"Unsupported JSON root type: {type(obj).__name__}\")\n\n    steps = None\n    for key in (\"steps\", \"records\", \"flow\", \"trajectory\"):\n        if key in obj:\n            steps = obj[key]\n            break\n\n    if steps is None and all(isinstance(v, dict) for v in obj.values()):\n        try:\n            steps = [v for _, v in sorted(obj.items(), key=lambda kv: int(str(kv[0])))]\n        except Exception:\n            steps = list(obj.values())\n\n    if not isinstance(steps, list):\n        raise ValueError(\"Could not locate a list of step records in JSON.\")\n\n    meta = dict(obj.get(\"meta\", {})) if isinstance(obj.get(\"meta\", {}), dict) else {}\n    meta.setdefault(\"source\", str(p))\n    meta.setdefault(\"container_keys\", sorted(obj.keys()))\n    return from_records(steps, name=nm, kind=kind, meta=meta)\n\n\ndef merge_datasets(*datasets: RGDataset, name: str = \"merged\") -> RGDataset:\n    \"\"\"Concatenate multiple datasets (useful for multi-run ensembles).\"\"\"\n    meta = {\"merged_from\": [d.name for d in datasets]}\n    steps: List[RGStep] = []\n    for di, d in enumerate(datasets):\n        for s in d.steps:\n            s2 = RGStep(**s.__dict__)\n            s2.meta = dict(s2.meta)\n            s2.meta.setdefault(\"run_index\", di)\n            steps.append(s2)\n    return RGDataset(name=name, kind=\"merged\", meta=meta, steps=steps)\n''')\n\nfinal_text = '\\n'.join(block.strip('\\n') for block in chunks).strip() + '\\n'\ntarget_path.write_text(final_text, encoding='utf-8')\nprint('FILE_WRITTEN:src/sf_gft_diagnostics/rg_io.py')\nprint('DIR_STATE:' + json.dumps(sorted(str(p.relative_to(Path(\"/mnt/data\"))) for p in target_path.parent.glob('*') if p.is_file())))\n",
      "container_id": "cntr_69499b97284881909e58fb047f14c1e300fdec2b43878464",
      "outputs": [
        {
          "type": "logs",
          "logs": "FILE_WRITTEN:src/sf_gft_diagnostics/rg_io.py\nDIR_STATE:[\"src/sf_gft_diagnostics/__init__.py\", \"src/sf_gft_diagnostics/metrics.py\", \"src/sf_gft_diagnostics/observables.py\", \"src/sf_gft_diagnostics/rg_io.py\", \"src/sf_gft_diagnostics/scaling.py\"]\n"
        }
      ]
    },
    {
      "id": "rs_09072f9ede3124240069499d2e67808195a48a99af947744ae",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "msg_09072f9ede3124240069499d2e9b048195abed1a5ed1ffd127",
      "type": "message",
      "status": "completed",
      "content": [
        {
          "type": "output_text",
          "annotations": [],
          "logprobs": [],
          "text": "Implemented `rg_io.py` with compact `RGDataset`/`RGStep` dataclasses plus JSON/JSONL loaders and normalization/merge utilities to unify tensor-network/lattice RG and GFT/spinfoam coarse-graining logs into a consistent in-memory trajectory format."
        }
      ],
      "role": "assistant"
    }
  ],
  "codeResults": []
}