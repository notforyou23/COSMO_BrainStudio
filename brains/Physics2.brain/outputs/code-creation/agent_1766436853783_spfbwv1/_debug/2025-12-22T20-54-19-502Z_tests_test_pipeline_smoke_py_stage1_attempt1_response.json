{
  "content": "Created `tests/test_pipeline_smoke.py` with deterministic end-to-end pytest smoke coverage for the protocol registry, simulation snapshot reproducibility, MAP-style parameter recovery via inference, and a full CLI run (protocols → simulate → infer) with validated JSON outputs.",
  "reasoning": "",
  "responseId": "resp_0c8a19b036380f61006949b297ea888197a753494876778842",
  "model": "gpt-5.2-2025-12-11",
  "usage": {
    "input_tokens": 16617,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 3179,
    "output_tokens_details": {
      "reasoning_tokens": 1829
    },
    "total_tokens": 19796
  },
  "hadError": false,
  "errorType": null,
  "webSearchSources": [],
  "citations": [],
  "output": [
    {
      "id": "rs_0c8a19b036380f61006949b29895408197a536fb1d1b0b9ebd",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "ci_0c8a19b036380f61006949b2999b8c8197967e75c70a37e9c0",
      "type": "code_interpreter_call",
      "status": "completed",
      "code": "import os, json, pathlib, textwrap, re\nbase=pathlib.Path(\"/mnt/data\")\nlist(base.iterdir())[:20]\n",
      "container_id": "cntr_6949aff88f24819097f165c497ce475e074f2320f5d510c4",
      "outputs": [
        {
          "type": "logs",
          "logs": "[PosixPath('/mnt/data/pyproject.toml'),\n PosixPath('/mnt/data/.github'),\n PosixPath('/mnt/data/README.md'),\n PosixPath('/mnt/data/src')]"
        }
      ]
    },
    {
      "id": "ci_0c8a19b036380f61006949b29bfec4819782fc85a592fb8df5",
      "type": "code_interpreter_call",
      "status": "completed",
      "code": "for p in [base/\"src\", base/\"tests\"]:\n    print(p, p.exists())\n",
      "container_id": "cntr_6949aff88f24819097f165c497ce475e074f2320f5d510c4",
      "outputs": [
        {
          "type": "logs",
          "logs": "/mnt/data/src True\n/mnt/data/tests False\n"
        }
      ]
    },
    {
      "id": "ci_0c8a19b036380f61006949b29e43908197b3340609fc009b59",
      "type": "code_interpreter_call",
      "status": "completed",
      "code": "# inspect src package\nfor p in (base/\"src\").iterdir():\n    print(p.name)\n",
      "container_id": "cntr_6949aff88f24819097f165c497ce475e074f2320f5d510c4",
      "outputs": [
        {
          "type": "logs",
          "logs": "dgpipe\n"
        }
      ]
    },
    {
      "id": "ci_0c8a19b036380f61006949b2a0af4881978673ef1d489c5abb",
      "type": "code_interpreter_call",
      "status": "completed",
      "code": "import pkgutil, sys\nsys.path.insert(0, str(base/\"src\"))\nimport dgpipe, inspect\ndgpipe.__file__\n",
      "container_id": "cntr_6949aff88f24819097f165c497ce475e074f2320f5d510c4",
      "outputs": [
        {
          "type": "logs",
          "logs": "'/mnt/data/src/dgpipe/__init__.py'"
        }
      ]
    },
    {
      "id": "ci_0c8a19b036380f61006949b2a317f88197a7ab0533b4aa1b3c",
      "type": "code_interpreter_call",
      "status": "completed",
      "code": "import pathlib, os\npkg=base/\"src/dgpipe\"\nfor p in pkg.rglob(\"*.py\"):\n    print(p.relative_to(base))\n",
      "container_id": "cntr_6949aff88f24819097f165c497ce475e074f2320f5d510c4",
      "outputs": [
        {
          "type": "logs",
          "logs": "src/dgpipe/__init__.py\nsrc/dgpipe/cli.py\nsrc/dgpipe/protocols.py\nsrc/dgpipe/models.py\nsrc/dgpipe/simulations.py\nsrc/dgpipe/inference.py\nsrc/dgpipe/io.py\n"
        }
      ]
    },
    {
      "id": "ci_0c8a19b036380f61006949b2a5e65481978299a01dacf1b119",
      "type": "code_interpreter_call",
      "status": "completed",
      "code": "# open these modules quickly\nfor name in [\"protocols\",\"simulations\",\"inference\",\"cli\",\"models\",\"io\"]:\n    path=pkg/f\"{name}.py\"\n    print(\"\\n==\", path.name, \"==\")\n    print(path.read_text()[:4000])\n",
      "container_id": "cntr_6949aff88f24819097f165c497ce475e074f2320f5d510c4",
      "outputs": [
        {
          "type": "logs",
          "logs": "\n== protocols.py ==\n\"\"\"Measurement-protocol library.\n\nProvides a small, prioritized, machine-readable registry of concrete measurement\nprotocols for correlators and entanglement diagnostics spanning analogue\nplatforms and astrophysical probes. The schema is designed for downstream\nsimulation and inference modules.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import asdict, dataclass, field\nfrom typing import Any, Dict, Iterable, List, Optional, Sequence\n@dataclass(frozen=True)\nclass Protocol:\n    \"\"\"Machine-readable measurement protocol.\n\n    Fields are JSON-serializable (lists/dicts/strings/numbers). `params` encodes\n    discrete-structure knobs (e.g., sprinkling density rho, discreteness length\n    ell_d, spectral dimension d_s) and how the measurement constrains them.\n    \"\"\"\n\n    id: str\n    title: str\n    platform: str  # e.g. \"BEC\", \"superconducting\", \"CMB\", \"GW\"\n    priority: int  # higher is earlier in a measurement campaign\n    observables: Sequence[str]\n    estimator: str  # concise description of the statistic returned\n    protocol_steps: Sequence[str]\n    data_requirements: Dict[str, Any]\n    systematics: Sequence[str]\n    params: Dict[str, Any] = field(default_factory=dict)\n    references: Sequence[str] = ()\n\n    def to_dict(self) -> Dict[str, Any]:\n        d = asdict(self)\n        d[\"schema_version\"] = \"1.0\"\n        return d\n# Curated protocols: concrete estimators + concrete readout steps.\n_PROTOCOLS: List[Protocol] = [\n    Protocol(\n        id=\"analogue.bec.density_g2_k\",\n        title=\"BEC: equal-time density correlator g2(k) for dispersive microstructure\",\n        platform=\"BEC / atom-chip\",\n        priority=100,\n        observables=[\"delta_n(x,t)\", \"g2(k,t)=<|delta_n_k|^2>/<n>^2\"],\n        estimator=\"Fit deviation of g2(k) from hydrodynamic prediction; extract k* crossover\",\n        protocol_steps=[\n            \"Prepare quasi-1D BEC; calibrate sound speed c_s and healing length xi\",\n            \"Quench/drive to generate phonon population; hold time t_hold\",\n            \"Acquire in-situ phase-contrast images; reconstruct density n(x)\",\n            \"FFT to n_k; compute ensemble-averaged g2(k) and covariance\",\n            \"Infer crossover scale k* and spectral tilt; map to ell_d via dispersion model\",\n        ],\n        data_requirements={\"shots\": 1_000, \"length_um\": 200, \"pixel_um\": 0.5, \"k_range\": [0.0, 8.0]},\n        systematics=[\n            \"Finite imaging resolution (MTF) and atom-number fluctuations\",\n            \"Trap inhomogeneity; windowing/edge effects in FFT\",\n            \"Nonlinear phonon-phonon interactions at high occupation\",\n        ],\n        params={\n            \"targets\": [\"ell_d\", \"alpha_disp\"],\n            \"likelihood\": \"Gaussian on binned g2(k); includes MTF nuisance\",\n            \"mapping\": \"k* ~ 1/ell_d after rescaling by xi and c_s\",\n        },\n        references=[\"analogue gravity density correlators\", \"dispersive phonons\"],\n    ),\n    Protocol(\n        id=\"analogue.sqc.two_mode_logneg\",\n        title=\"Superconducting circuit: two-mode squeezing and logarithmic negativity\",\n        platform=\"superconducting microwave / cQED\",\n        priority=95,\n        observables=[\"I/Q quadratures\", \"covariance matrix V\", \"E_N\"],\n        estimator=\"Compute Gaussian log-negativity from calibrated quadrature covariance\",\n        protocol_steps=[\n            \"Implement parametric modulation to realize two-mode squeezing of itinerant modes\",\n            \"Heterodyne-detect both outputs; calibrate gain, added noise, and phase\",\n            \"Estimate 4x4 covariance matrix V from time streams; bootstrap errors\",\n            \"Compute log-negativity E_N(V) and compare vs frequency-dependent dispersion\",\n        ],\n        data_requirements={\"bandwidth_MHz\": 50, \"records\": 10_000, \"calibration\": [\"gain\", \"noise\", \"phase\"]},\n        systematics=[\n            \"Added amplifier noise and drift; imperfect mode matching\",\n            \"Finite detection bandwidth (filtering) biases V\",\n            \"Non-Gaussian tails from r\n\n== simulations.py ==\n\"\"\"dgpipe.simulations\n\nControlled numerical + semi-analytical simulators used to quantify finite-size,\ndiscretization, and dispersive systematics for measurement protocols and to\nproduce synthetic datasets for inference tests.\n\nThe models here are intentionally lightweight: a free scalar field in 1D with\nperiodic boundary conditions and a causal-set/discrete-microstructure inspired\nmodified dispersion relation.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, asdict\nfrom typing import Any, Dict, Iterable, Literal, Mapping, Optional, Tuple\n\nimport numpy as np\n\n\nProtocolName = Literal[\n    \"equal_time_correlator\",\n    \"time_correlator\",\n    \"logneg_two_mode\",\n]\n@dataclass(frozen=True)\nclass DiscreteParams:\n    \"\"\"Parameters encoding discrete microstructure effects.\n\n    alpha, power: control a leading-order dispersive correction\n        omega(k) = sqrt(k^2 + m^2) * (1 + alpha * (a|k|)^power)\n    a: microscopic length/spacing, also sets UV cutoff k_max ~ pi/a.\n    \"\"\"\n\n    a: float = 1.0\n    alpha: float = 0.0\n    power: int = 2\n    m: float = 0.0\n\n\n@dataclass(frozen=True)\nclass SimConfig:\n    \"\"\"Finite-size/discretization controls.\"\"\"\n\n    L: float = 64.0          # system length\n    N_modes: int = 256       # number of Fourier modes (finite-size)\n    k_cut: Optional[float] = None  # optional additional cutoff (<= pi/a)\n\n\n@dataclass(frozen=True)\nclass NoiseModel:\n    \"\"\"Simple i.i.d. Gaussian measurement noise.\"\"\"\n\n    sigma: float = 1e-3\n    seed: int = 0\ndef _k_grid(cfg: SimConfig, dp: DiscreteParams) -> np.ndarray:\n    n = np.arange(-cfg.N_modes // 2, cfg.N_modes // 2, dtype=float)\n    k = 2.0 * np.pi * n / cfg.L\n    k_max = np.pi / max(dp.a, 1e-12)\n    if cfg.k_cut is not None:\n        k_max = min(k_max, float(cfg.k_cut))\n    return k[np.abs(k) <= k_max]\n\n\ndef _omega(k: np.ndarray, dp: DiscreteParams) -> np.ndarray:\n    w0 = np.sqrt(k * k + dp.m * dp.m)\n    corr = 1.0 + dp.alpha * np.power(dp.a * np.abs(k), dp.power)\n    return w0 * corr\n\n\ndef _wightman(t: float, x: np.ndarray, cfg: SimConfig, dp: DiscreteParams) -> np.ndarray:\n    \"\"\"Discrete-mode approximation to <phi(t,x) phi(0,0)> in vacuum.\"\"\"\n    k = _k_grid(cfg, dp)\n    w = _omega(k, dp)\n    phase = np.outer(k, x) - (w[:, None] * t)\n    # (1/L) sum_k (e^{i(kx - wt)})/(2w)\n    return (np.cos(phase) / (2.0 * w[:, None])).sum(axis=0) / cfg.L\n\n\ndef _pi_pi(t: float, x: np.ndarray, cfg: SimConfig, dp: DiscreteParams) -> np.ndarray:\n    \"\"\"Discrete-mode approximation to <pi(t,x) pi(0,0)> with pi=dot(phi).\"\"\"\n    k = _k_grid(cfg, dp)\n    w = _omega(k, dp)\n    phase = np.outer(k, x) - (w[:, None] * t)\n    return ((w[:, None] * np.cos(phase)) / 2.0).sum(axis=0) / cfg.L\ndef _add_noise(y: np.ndarray, noise: NoiseModel) -> Tuple[np.ndarray, np.ndarray]:\n    rng = np.random.default_rng(noise.seed)\n    eps = rng.normal(0.0, noise.sigma, size=y.shape)\n    return y + eps, np.full_like(y, noise.sigma, dtype=float)\n\n\ndef simulate_equal_time_correlator(\n    x: Iterable[float],\n    *,\n    cfg: SimConfig,\n    dp: DiscreteParams,\n    noise: Optional[NoiseModel] = None,\n) -> Dict[str, Any]:\n    x = np.asarray(list(x), dtype=float)\n    y = _wightman(0.0, x, cfg, dp)\n    if noise is not None:\n        y, yerr = _add_noise(y, noise)\n    else:\n        yerr = np.zeros_like(y)\n    return {\n        \"protocol\": \"equal_time_correlator\",\n        \"x\": x.tolist(),\n        \"t\": 0.0,\n        \"y\": y.tolist(),\n        \"yerr\": yerr.tolist(),\n        \"meta\": {\"sim\": asdict(cfg), \"discrete\": asdict(dp), \"noise\": None if noise is None else asdict(noise)},\n    }\n\n\ndef simulate_time_correlator(\n    t: Iterable[float],\n    *,\n    x: float = 0.0,\n    cfg: SimConfig,\n    dp: DiscreteParams,\n    noise: Optional[NoiseModel] = None,\n) -> Dict[str, Any]:\n    t = np.asarray(list(t), dtype=float)\n    y = np.array([_wightman(float(tt), np.array([x]), cfg, dp)[0] for tt in t], dtype=float)\n    if noise is not None:\n        y, yerr = _add_noise(y, noise)\n    else:\n        yerr = np.zeros_like(y)\n   \n\n== inference.py ==\n\"\"\"dgpipe.inference\n\nLightweight statistical inference utilities to translate measured correlator /\nentanglement data into constraints on discrete-structure parameters.\n\nDesign goals: minimal dependencies (numpy only), support for Gaussian\nlikelihoods, additive linear systematics with analytic marginalization, and a\nbasic Metropolis-Hastings posterior sampler.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Callable, Dict, Iterable, Mapping, Optional, Sequence, Tuple, Union\n\nimport numpy as np\n\nArray = np.ndarray\nPriorSpec = Union[Tuple[str, float, float], Tuple[str, Array, Array]]  # uniform/gaussian\ndef _as_1d(x: Union[Array, Sequence[float]]) -> Array:\n    x = np.asarray(x, dtype=float)\n    return x.reshape(-1)\n\n\ndef _cov_from_sigma(sigma: Union[float, Array], n: int) -> Array:\n    s = np.asarray(sigma, dtype=float)\n    if s.ndim == 0:\n        return np.eye(n) * float(s) ** 2\n    s = s.reshape(-1)\n    if s.size != n:\n        raise ValueError(f\"sigma has size {s.size}, expected {n}\")\n    return np.diag(s ** 2)\n\n\ndef _loglike_gaussian(resid: Array, cov: Array) -> float:\n    cov = np.asarray(cov, dtype=float)\n    n = resid.size\n    try:\n        L = np.linalg.cholesky(cov)\n        sol = np.linalg.solve(L, resid)\n        quad = float(sol @ sol)\n        logdet = 2.0 * float(np.log(np.diag(L)).sum())\n    except np.linalg.LinAlgError:\n        covi = np.linalg.pinv(cov)\n        quad = float(resid @ covi @ resid)\n        sign, logdet = np.linalg.slogdet(cov + 1e-12 * np.eye(n))\n        logdet = float(logdet) if sign > 0 else float(np.log(np.abs(np.linalg.det(cov) + 1e-30)))\n    return -0.5 * (quad + logdet + n * np.log(2.0 * np.pi))\ndef marginal_loglike_additive_linear(\n    y: Array,\n    mu: Array,\n    cov_y: Array,\n    A: Optional[Array] = None,\n    nu_mean: Optional[Array] = None,\n    nu_cov: Optional[Array] = None,\n) -> float:\n    \"\"\"Gaussian log-likelihood with analytic marginalization over nuisance.\n\n    Model: y = mu(theta) + A nu + eps,  eps~N(0,cov_y),  nu~N(nu_mean,nu_cov).\n    The marginalized distribution is Gaussian with mean mu + A nu_mean and\n    covariance cov_y + A nu_cov A^T.\n    \"\"\"\n    y = _as_1d(y)\n    mu = _as_1d(mu)\n    if y.size != mu.size:\n        raise ValueError(\"y and mu must have same length\")\n    cov_eff = np.asarray(cov_y, dtype=float)\n    mean_eff = mu\n    if A is not None and nu_cov is not None:\n        A = np.asarray(A, dtype=float)\n        nu_mean = np.zeros(A.shape[1]) if nu_mean is None else _as_1d(nu_mean)\n        nu_cov = np.asarray(nu_cov, dtype=float)\n        mean_eff = mu + A @ nu_mean\n        cov_eff = cov_eff + A @ nu_cov @ A.T\n    return _loglike_gaussian(y - mean_eff, cov_eff)\ndef logprior(theta: Array, priors: Optional[Mapping[str, PriorSpec]] = None, names: Optional[Sequence[str]] = None) -> float:\n    \"\"\"Compute log prior for parameter vector theta.\n\n    priors maps parameter name -> ('uniform', low, high) or ('gaussian', mean, cov).\n    For gaussian: mean and cov can be scalars (interpreted as diagonal variance) or arrays.\n    \"\"\"\n    if not priors:\n        return 0.0\n    theta = _as_1d(theta)\n    if names is None:\n        names = [f\"p{i}\" for i in range(theta.size)]\n    lp = 0.0\n    for i, nm in enumerate(names):\n        if nm not in priors:\n            continue\n        spec = priors[nm]\n        if spec[0] == \"uniform\":\n            lo, hi = float(spec[1]), float(spec[2])\n            if not (lo <= theta[i] <= hi):\n                return -np.inf\n        elif spec[0] == \"gaussian\":\n            mean = np.asarray(spec[1], dtype=float)\n            cov = np.asarray(spec[2], dtype=float)\n            m = float(mean) if mean.ndim == 0 else float(mean.reshape(-1)[0])\n            v = float(cov) if cov.ndim == 0 else float(np.diag(cov).reshape(-1)[0])\n            lp += -0.5 * ((theta[i] - m) ** 2 / v + np.log(2.0 * np.pi * v))\n        else:\n            raise ValueError(f\"Unknown prior type: {spec[0]}\")\n    return float(lp)\n@dataclass(frozen=True)\nclass\n\n== cli.py ==\nfrom __future__ import annotations\nimport argparse, json, math, os, platform, random, sys, hashlib, datetime, subprocess\nfrom pathlib import Path\ndef _utcnow() -> str:\n    return datetime.datetime.now(datetime.timezone.utc).isoformat()\n\ndef _sha256_bytes(b: bytes) -> str:\n    return hashlib.sha256(b).hexdigest()\n\ndef _maybe_git_rev(cwd: Path) -> str | None:\n    try:\n        out = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], cwd=str(cwd), stderr=subprocess.DEVNULL)\n        return out.decode().strip()\n    except Exception:\n        return None\n\ndef _provenance(argv: list[str], outdir: Path) -> dict:\n    return {\n        \"timestamp_utc\": _utcnow(),\n        \"argv\": argv,\n        \"python\": sys.version.split()[0],\n        \"platform\": {\"system\": platform.system(), \"release\": platform.release(), \"machine\": platform.machine()},\n        \"git_rev\": _maybe_git_rev(Path.cwd()),\n        \"outdir\": str(outdir),\n    }\n\ndef _write_json(path: Path, obj: dict) -> None:\n    data = json.dumps(obj, indent=2, sort_keys=True).encode(\"utf-8\")\n    path.write_bytes(data)\ndef generate_protocols() -> dict:\n    # Prioritized protocols for correlators + entanglement diagnostics across analogue/astro probes.\n    return {\n        \"version\": 1,\n        \"parameters\": {\n            \"ell\": \"discreteness length scale (effective) controlling dispersive corrections\",\n            \"rho\": \"sprinkling density / microstructure density proxy\",\n            \"alpha\": \"dimensionless dispersion strength\",\n        },\n        \"protocols\": [\n            {\n                \"priority\": 1,\n                \"name\": \"Equal-time two-point correlator C(x) in analogue QFT\",\n                \"platforms\": [\"BEC phonons\", \"optical waveguides\", \"superconducting circuits\"],\n                \"observable\": \"C(x)=<phi(0)phi(x)> or density-density correlator\",\n                \"measurement\": \"homodyne/Bragg imaging; extract C(x) and Fourier spectrum S(k)\",\n                \"signature\": \"UV roll-off and oscillatory residuals consistent with discrete spectrum / modified dispersion\",\n                \"systematics\": [\"finite-size\", \"trap inhomogeneity\", \"detector resolution\", \"thermal background\"],\n            },\n            {\n                \"priority\": 2,\n                \"name\": \"Unequal-time correlator C(t) and retarded Green's function\",\n                \"platforms\": [\"optomechanical arrays\", \"cold-atom simulators\"],\n                \"observable\": \"C(t)=<phi(0)phi(t)>, G_R(t)\",\n                \"measurement\": \"pump-probe; quench then read out temporal correlations\",\n                \"signature\": \"phase slip / beating from discrete modes; dispersive group-delay shifts\",\n                \"systematics\": [\"dispersion calibration\", \"sampling jitter\", \"bath coupling\"],\n            },\n            {\n                \"priority\": 3,\n                \"name\": \"Entanglement diagnostics via mutual information / negativity proxy\",\n                \"platforms\": [\"continuous-variable photonics\", \"cQED\", \"ion chains\"],\n                \"observable\": \"I(A:B) from covariance matrix; logarithmic negativity if accessible\",\n                \"measurement\": \"Gaussian tomography / quadrature readout; build covariance blocks\",\n                \"signature\": \"scale-dependent suppression/enhancement across partitions tracking ell, rho\",\n                \"systematics\": [\"loss\", \"mode mismatch\", \"finite squeezing\", \"classical correlations\"],\n            },\n            {\n                \"priority\": 4,\n                \"name\": \"Astrophysical time-of-flight dispersion constraints\",\n                \"platforms\": [\"GRB/FRB timing\", \"GW multi-messenger\"],\n                \"observable\": \"energy-dependent arrival delays\",\n                \"measurement\": \"cross-band timing; hierarchical population model\",\n                \"signature\": \"Δt ∝ alpha*ell^2*E^2*D (toy), bounded by non-detection\",\n                \"systematics\": [\"intrinsic source lags\", \"plasma dispersion\", \"selection biases\"],\n            },\n        ],\n    }\ndef simulate(outdir: Path, n: int\n\n== models.py ==\n\"\"\"Discrete-gravity-inspired effective free-field models and observables.\n\nThis module provides (i) modified dispersion/propagators motivated by discrete\nmicrostructure (causal sets, discrete spectra) and (ii) mappings to measurable\nGaussian correlators and simple entanglement diagnostics in analogue platforms.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Callable, Optional\n\nimport numpy as np\ndef coth(x: np.ndarray) -> np.ndarray:\n    x = np.asarray(x, dtype=float)\n    out = np.empty_like(x)\n    small = np.abs(x) < 1e-6\n    out[small] = 1.0 / x[small] + x[small] / 3.0  # series for stability\n    out[~small] = np.cosh(x[~small]) / np.sinh(x[~small])\n    return out\n\n\n@dataclass(frozen=True)\nclass Dispersion:\n    \"\"\"Callable dispersion ω(k) with physical metadata.\"\"\"\n\n    name: str\n    omega: Callable[[np.ndarray], np.ndarray]\n    mass: float = 0.0\n    params: tuple = ()\n\n    def __call__(self, k: np.ndarray) -> np.ndarray:\n        return self.omega(np.asarray(k, dtype=float))\ndef relativistic_dispersion(mass: float = 0.0, c: float = 1.0) -> Dispersion:\n    def omega(k: np.ndarray) -> np.ndarray:\n        return np.sqrt((c * k) ** 2 + mass**2)\n\n    return Dispersion(name=\"relativistic\", omega=omega, mass=mass, params=(c,))\n\n\ndef lifshitz_dispersion(\n    mass: float = 0.0, c: float = 1.0, z: int = 2, k0: float = 1.0\n) -> Dispersion:\n    \"\"\"ω^2 = m^2 + c^2 k^2 + (k/k0)^(2z) k0^2; mimics UV discreteness.\"\"\"\n\n    def omega(k: np.ndarray) -> np.ndarray:\n        return np.sqrt(mass**2 + (c * k) ** 2 + (np.abs(k) / k0) ** (2 * z) * k0**2)\n\n    return Dispersion(name=f\"lifshitz_z{z}\", omega=omega, mass=mass, params=(c, z, k0))\n\n\ndef causalset_like_dispersion(\n    mass: float = 0.0, ell: float = 1.0, alpha: float = 1.0, p: int = 2\n) -> Dispersion:\n    \"\"\"Smooth nonlocal/causal-set inspired UV correction.\n\n    ω^2 = m^2 + k^2 * (1 + α (ℓ k)^p)/(1 + (ℓ k)^p), interpolating IR→UV.\n    \"\"\"\n\n    def omega(k: np.ndarray) -> np.ndarray:\n        kk = np.abs(k)\n        u = (ell * kk) ** p\n        eff = kk**2 * (1.0 + alpha * u) / (1.0 + u)\n        return np.sqrt(mass**2 + eff)\n\n    return Dispersion(name=\"causalset_like\", omega=omega, mass=mass, params=(ell, alpha, p))\ndef finite_volume_modes_1d(L: float, nmax: int, periodic: bool = True) -> np.ndarray:\n    \"\"\"Discrete momenta k_n for a 1D box; periodic uses k=2πn/L.\"\"\"\n    n = np.arange(-nmax, nmax + 1, dtype=int)\n    if periodic:\n        return 2.0 * np.pi * n / L\n    return np.pi * n / L  # e.g. Dirichlet/Neumann up to phase conventions\n\n\ndef thermal_occupation(omega: np.ndarray, T: float) -> np.ndarray:\n    if T <= 0:\n        return np.zeros_like(omega, dtype=float)\n    x = omega / T\n    x = np.clip(x, 1e-12, 7e2)\n    return 1.0 / (np.exp(x) - 1.0)\n\n\ndef power_spectrum_equal_time(omega: np.ndarray, T: float = 0.0) -> np.ndarray:\n    \"\"\"P(k)=⟨|φ_k|^2⟩ for a free scalar at temperature T.\"\"\"\n    omega = np.asarray(omega, dtype=float)\n    if T <= 0:\n        return 1.0 / (2.0 * omega)\n    return 0.5 / omega * coth(omega / (2.0 * T))\ndef wightman_1d_sum(\n    dispersion: Dispersion,\n    x: float,\n    t: float,\n    L: float,\n    nmax: int,\n    T: float = 0.0,\n    periodic: bool = True,\n) -> complex:\n    \"\"\"Finite-volume Wightman function in 1D from discrete mode sum.\"\"\"\n    k = finite_volume_modes_1d(L=L, nmax=nmax, periodic=periodic)\n    w = dispersion(k)\n    n = thermal_occupation(w, T)\n    phase = np.exp(1j * k * x)\n    term = (n + 1.0) * np.exp(-1j * w * t) + n * np.exp(1j * w * t)\n    return np.sum(phase * term / (2.0 * w)) / L\n\n\ndef equal_time_correlators_1d(\n    dispersion: Dispersion, x: float, L: float, nmax: int, T: float = 0.0\n) -> tuple[float, float]:\n    \"\"\"Return (⟨φ(0)φ(x)⟩, ⟨π(0)π(x)⟩) at equal time in 1D finite volume.\"\"\"\n    k = finite_volume_modes_1d(L=L, nmax=nmax, periodic=True)\n    w = dispersion(k)\n    P = power_spectrum_equal_time(w, T)\n    cxx = float(np.sum(np.cos(k * x) * P) / L)\n    cpp = float(np.sum(np.cos(k * x) * (w**2) *\n\n== io.py ==\n\"\"\"dgpipe.io\n\nRobust, reproducible I/O helpers for configs, datasets, results, and provenance.\nSupports JSON / CSV / NPZ with lightweight schema validation and deterministic seeding.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Mapping, MutableMapping, Sequence\nimport csv\nimport hashlib\nimport json\nimport os\nimport platform\nimport random\nimport sys\nimport time\n\nimport numpy as np\ndef _atomic_write_text(path: Path, text: str, encoding: str = \"utf-8\") -> None:\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    tmp = path.with_suffix(path.suffix + \".tmp\")\n    tmp.write_text(text, encoding=encoding)\n    os.replace(tmp, path)\n\n\ndef _json_dumps_deterministic(obj: Any) -> str:\n    return json.dumps(obj, sort_keys=True, indent=2, ensure_ascii=False) + \"\\n\"\n\n\ndef load_json(path: str | Path) -> Any:\n    with Path(path).open(\"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\n\n\ndef save_json(path: str | Path, obj: Any) -> None:\n    _atomic_write_text(Path(path), _json_dumps_deterministic(obj))\n\n\ndef load_csv_dicts(path: str | Path) -> list[dict[str, str]]:\n    with Path(path).open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n        return list(csv.DictReader(f))\n\n\ndef save_csv_dicts(path: str | Path, rows: Sequence[Mapping[str, Any]]) -> None:\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    rows = list(rows)\n    fieldnames = sorted({k for r in rows for k in r.keys()})\n    tmp = path.with_suffix(path.suffix + \".tmp\")\n    with tmp.open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n        w = csv.DictWriter(f, fieldnames=fieldnames)\n        w.writeheader()\n        for r in rows:\n            w.writerow({k: r.get(k, \"\") for k in fieldnames})\n    os.replace(tmp, path)\n\n\ndef save_npz(path: str | Path, **arrays: Any) -> None:\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    tmp = path.with_suffix(path.suffix + \".tmp\")\n    np.savez_compressed(tmp, **arrays)\n    os.replace(tmp, path)\n\n\ndef load_npz(path: str | Path) -> dict[str, np.ndarray]:\n    with np.load(Path(path), allow_pickle=False) as z:\n        return {k: z[k] for k in z.files}\n@dataclass(frozen=True)\nclass Schema:\n    \"\"\"Very small schema: mapping key -> type / nested Schema / (type, ...) / list[Schema].\"\"\"\n\n    spec: Mapping[str, Any]\n\n    def validate(self, obj: Any, *, where: str = \"root\") -> None:\n        if not isinstance(obj, Mapping):\n            raise TypeError(f\"{where}: expected mapping, got {type(obj).__name__}\")\n        for key, rule in self.spec.items():\n            if key not in obj:\n                raise KeyError(f\"{where}: missing required key '{key}'\")\n            _validate_rule(obj[key], rule, where=f\"{where}.{key}\")\n\n\ndef _validate_rule(val: Any, rule: Any, *, where: str) -> None:\n    if isinstance(rule, Schema):\n        rule.validate(val, where=where)\n        return\n    if isinstance(rule, list):\n        if len(rule) != 1 or not isinstance(rule[0], (type, Schema, tuple)):\n            raise TypeError(f\"{where}: invalid list rule\")\n        if not isinstance(val, Sequence) or isinstance(val, (str, bytes, bytearray)):\n            raise TypeError(f\"{where}: expected sequence, got {type(val).__name__}\")\n        for i, v in enumerate(val):\n            _validate_rule(v, rule[0], where=f\"{where}[{i}]\")\n        return\n    if isinstance(rule, tuple):\n        if not isinstance(val, rule):\n            exp = \",\".join(t.__name__ for t in rule)\n            raise TypeError(f\"{where}: expected one of ({exp}), got {type(val).__name__}\")\n        return\n    if isinstance(rule, type):\n        if not isinstance(val, rule):\n            raise TypeError(f\"{where}: expected {rule.__name__}, got {type(val).__name__}\")\n        return\n    raise TypeError(f\"{where}: invalid schema rule {rule!r}\")\n\n\ndef load_config(path: str | Path, schema: Schema | None = None) -> dict[str, Any]:\n    cfg = load_json(path)\n    if schema is not None:\n        s\n"
        }
      ]
    },
    {
      "id": "rs_0c8a19b036380f61006949b2a90a988197a6b2ca6c0ece5d52",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "ci_0c8a19b036380f61006949b2a972e081978a0385ac1990d4cd",
      "type": "code_interpreter_call",
      "status": "completed",
      "code": "print((pkg/\"inference.py\").read_text()[0:4000])\n",
      "container_id": "cntr_6949aff88f24819097f165c497ce475e074f2320f5d510c4",
      "outputs": [
        {
          "type": "logs",
          "logs": "\"\"\"dgpipe.inference\n\nLightweight statistical inference utilities to translate measured correlator /\nentanglement data into constraints on discrete-structure parameters.\n\nDesign goals: minimal dependencies (numpy only), support for Gaussian\nlikelihoods, additive linear systematics with analytic marginalization, and a\nbasic Metropolis-Hastings posterior sampler.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Callable, Dict, Iterable, Mapping, Optional, Sequence, Tuple, Union\n\nimport numpy as np\n\nArray = np.ndarray\nPriorSpec = Union[Tuple[str, float, float], Tuple[str, Array, Array]]  # uniform/gaussian\ndef _as_1d(x: Union[Array, Sequence[float]]) -> Array:\n    x = np.asarray(x, dtype=float)\n    return x.reshape(-1)\n\n\ndef _cov_from_sigma(sigma: Union[float, Array], n: int) -> Array:\n    s = np.asarray(sigma, dtype=float)\n    if s.ndim == 0:\n        return np.eye(n) * float(s) ** 2\n    s = s.reshape(-1)\n    if s.size != n:\n        raise ValueError(f\"sigma has size {s.size}, expected {n}\")\n    return np.diag(s ** 2)\n\n\ndef _loglike_gaussian(resid: Array, cov: Array) -> float:\n    cov = np.asarray(cov, dtype=float)\n    n = resid.size\n    try:\n        L = np.linalg.cholesky(cov)\n        sol = np.linalg.solve(L, resid)\n        quad = float(sol @ sol)\n        logdet = 2.0 * float(np.log(np.diag(L)).sum())\n    except np.linalg.LinAlgError:\n        covi = np.linalg.pinv(cov)\n        quad = float(resid @ covi @ resid)\n        sign, logdet = np.linalg.slogdet(cov + 1e-12 * np.eye(n))\n        logdet = float(logdet) if sign > 0 else float(np.log(np.abs(np.linalg.det(cov) + 1e-30)))\n    return -0.5 * (quad + logdet + n * np.log(2.0 * np.pi))\ndef marginal_loglike_additive_linear(\n    y: Array,\n    mu: Array,\n    cov_y: Array,\n    A: Optional[Array] = None,\n    nu_mean: Optional[Array] = None,\n    nu_cov: Optional[Array] = None,\n) -> float:\n    \"\"\"Gaussian log-likelihood with analytic marginalization over nuisance.\n\n    Model: y = mu(theta) + A nu + eps,  eps~N(0,cov_y),  nu~N(nu_mean,nu_cov).\n    The marginalized distribution is Gaussian with mean mu + A nu_mean and\n    covariance cov_y + A nu_cov A^T.\n    \"\"\"\n    y = _as_1d(y)\n    mu = _as_1d(mu)\n    if y.size != mu.size:\n        raise ValueError(\"y and mu must have same length\")\n    cov_eff = np.asarray(cov_y, dtype=float)\n    mean_eff = mu\n    if A is not None and nu_cov is not None:\n        A = np.asarray(A, dtype=float)\n        nu_mean = np.zeros(A.shape[1]) if nu_mean is None else _as_1d(nu_mean)\n        nu_cov = np.asarray(nu_cov, dtype=float)\n        mean_eff = mu + A @ nu_mean\n        cov_eff = cov_eff + A @ nu_cov @ A.T\n    return _loglike_gaussian(y - mean_eff, cov_eff)\ndef logprior(theta: Array, priors: Optional[Mapping[str, PriorSpec]] = None, names: Optional[Sequence[str]] = None) -> float:\n    \"\"\"Compute log prior for parameter vector theta.\n\n    priors maps parameter name -> ('uniform', low, high) or ('gaussian', mean, cov).\n    For gaussian: mean and cov can be scalars (interpreted as diagonal variance) or arrays.\n    \"\"\"\n    if not priors:\n        return 0.0\n    theta = _as_1d(theta)\n    if names is None:\n        names = [f\"p{i}\" for i in range(theta.size)]\n    lp = 0.0\n    for i, nm in enumerate(names):\n        if nm not in priors:\n            continue\n        spec = priors[nm]\n        if spec[0] == \"uniform\":\n            lo, hi = float(spec[1]), float(spec[2])\n            if not (lo <= theta[i] <= hi):\n                return -np.inf\n        elif spec[0] == \"gaussian\":\n            mean = np.asarray(spec[1], dtype=float)\n            cov = np.asarray(spec[2], dtype=float)\n            m = float(mean) if mean.ndim == 0 else float(mean.reshape(-1)[0])\n            v = float(cov) if cov.ndim == 0 else float(np.diag(cov).reshape(-1)[0])\n            lp += -0.5 * ((theta[i] - m) ** 2 / v + np.log(2.0 * np.pi * v))\n        else:\n            raise ValueError(f\"Unknown prior type: {spec[0]}\")\n    return float(lp)\n@dataclass(frozen=True)\nclass\n"
        }
      ]
    },
    {
      "id": "rs_0c8a19b036380f61006949b2ab8ea0819789761ddaa15db4f6",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "ci_0c8a19b036380f61006949b2ac37588197afd274a0439bdf2b",
      "type": "code_interpreter_call",
      "status": "completed",
      "code": "text=(pkg/\"inference.py\").read_text()\nprint(\"len\",len(text))\nprint(text[4000:8000])\n",
      "container_id": "cntr_6949aff88f24819097f165c497ce475e074f2320f5d510c4",
      "outputs": [
        {
          "type": "logs",
          "logs": "len 7095\n InferenceProblem:\n    \"\"\"Bundle a dataset, a forward model, and systematics for inference.\"\"\"\n\n    x: Array\n    y: Array\n    sigma: Union[float, Array]\n    model: Callable[[Array, Array], Array]  # model(theta, x)->mu\n    param_names: Tuple[str, ...] = ()\n    priors: Optional[Dict[str, PriorSpec]] = None\n    A_nuisance: Optional[Array] = None\n    nuisance_mean: Optional[Array] = None\n    nuisance_cov: Optional[Array] = None\n\n    def loglike(self, theta: Array) -> float:\n        mu = _as_1d(self.model(_as_1d(theta), np.asarray(self.x)))\n        y = _as_1d(self.y)\n        cov_y = _cov_from_sigma(self.sigma, y.size)\n        return marginal_loglike_additive_linear(\n            y=y, mu=mu, cov_y=cov_y, A=self.A_nuisance, nu_mean=self.nuisance_mean, nu_cov=self.nuisance_cov\n        )\n\n    def logpost(self, theta: Array) -> float:\n        lp = logprior(theta, self.priors, self.param_names or None)\n        if not np.isfinite(lp):\n            return -np.inf\n        return lp + self.loglike(theta)\ndef metropolis_hastings(\n    logp: Callable[[Array], float],\n    x0: Array,\n    nsteps: int,\n    proposal_scale: Union[float, Array] = 0.1,\n    rng: Optional[np.random.Generator] = None,\n) -> Tuple[Array, Array, float]:\n    \"\"\"Random-walk Metropolis-Hastings sampler (Gaussian proposals).\"\"\"\n    rng = np.random.default_rng() if rng is None else rng\n    x = _as_1d(x0).copy()\n    d = x.size\n    s = np.asarray(proposal_scale, dtype=float)\n    step = (np.ones(d) * float(s)) if s.ndim == 0 else s.reshape(-1)\n    if step.size != d:\n        raise ValueError(\"proposal_scale has wrong dimension\")\n    chain = np.empty((nsteps, d), dtype=float)\n    lps = np.empty(nsteps, dtype=float)\n    lp = float(logp(x))\n    acc = 0\n    for t in range(nsteps):\n        prop = x + rng.normal(scale=step, size=d)\n        lpp = float(logp(prop))\n        if np.isfinite(lpp) and np.log(rng.random()) < (lpp - lp):\n            x, lp = prop, lpp\n            acc += 1\n        chain[t], lps[t] = x, lp\n    return chain, lps, acc / max(1, nsteps)\ndef summarize_chain(chain: Array, qs: Sequence[float] = (0.16, 0.5, 0.84)) -> Dict[str, Array]:\n    chain = np.asarray(chain, dtype=float)\n    if chain.ndim != 2:\n        raise ValueError(\"chain must be (nsteps, ndim)\")\n    mean = chain.mean(axis=0)\n    cov = np.cov(chain.T)\n    quant = np.quantile(chain, qs, axis=0)\n    return {\"mean\": mean, \"cov\": cov, \"quantiles\": quant}\n\n\ndef map_fit_random(\n    logpost: Callable[[Array], float],\n    bounds: Sequence[Tuple[float, float]],\n    nsamples: int = 2000,\n    rng: Optional[np.random.Generator] = None,\n) -> Tuple[Array, float]:\n    \"\"\"Crude frequentist/MAP fit by random search within bounds.\"\"\"\n    rng = np.random.default_rng() if rng is None else rng\n    lo = np.array([b[0] for b in bounds], dtype=float)\n    hi = np.array([b[1] for b in bounds], dtype=float)\n    best_x = lo.copy()\n    best_lp = -np.inf\n    for _ in range(int(nsamples)):\n        x = lo + (hi - lo) * rng.random(size=lo.size)\n        lp = float(logpost(x))\n        if lp > best_lp:\n            best_x, best_lp = x, lp\n    return best_x, best_lp\n\n"
        }
      ]
    },
    {
      "id": "rs_0c8a19b036380f61006949b2ae5c348197916cc8fc843cdcf9",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "ci_0c8a19b036380f61006949b2afa5f481979d0cd77b6daad600",
      "type": "code_interpreter_call",
      "status": "completed",
      "code": "cli_text=(pkg/\"cli.py\").read_text()\nprint(cli_text[0:4000])\n",
      "container_id": "cntr_6949aff88f24819097f165c497ce475e074f2320f5d510c4",
      "outputs": [
        {
          "type": "logs",
          "logs": "from __future__ import annotations\nimport argparse, json, math, os, platform, random, sys, hashlib, datetime, subprocess\nfrom pathlib import Path\ndef _utcnow() -> str:\n    return datetime.datetime.now(datetime.timezone.utc).isoformat()\n\ndef _sha256_bytes(b: bytes) -> str:\n    return hashlib.sha256(b).hexdigest()\n\ndef _maybe_git_rev(cwd: Path) -> str | None:\n    try:\n        out = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], cwd=str(cwd), stderr=subprocess.DEVNULL)\n        return out.decode().strip()\n    except Exception:\n        return None\n\ndef _provenance(argv: list[str], outdir: Path) -> dict:\n    return {\n        \"timestamp_utc\": _utcnow(),\n        \"argv\": argv,\n        \"python\": sys.version.split()[0],\n        \"platform\": {\"system\": platform.system(), \"release\": platform.release(), \"machine\": platform.machine()},\n        \"git_rev\": _maybe_git_rev(Path.cwd()),\n        \"outdir\": str(outdir),\n    }\n\ndef _write_json(path: Path, obj: dict) -> None:\n    data = json.dumps(obj, indent=2, sort_keys=True).encode(\"utf-8\")\n    path.write_bytes(data)\ndef generate_protocols() -> dict:\n    # Prioritized protocols for correlators + entanglement diagnostics across analogue/astro probes.\n    return {\n        \"version\": 1,\n        \"parameters\": {\n            \"ell\": \"discreteness length scale (effective) controlling dispersive corrections\",\n            \"rho\": \"sprinkling density / microstructure density proxy\",\n            \"alpha\": \"dimensionless dispersion strength\",\n        },\n        \"protocols\": [\n            {\n                \"priority\": 1,\n                \"name\": \"Equal-time two-point correlator C(x) in analogue QFT\",\n                \"platforms\": [\"BEC phonons\", \"optical waveguides\", \"superconducting circuits\"],\n                \"observable\": \"C(x)=<phi(0)phi(x)> or density-density correlator\",\n                \"measurement\": \"homodyne/Bragg imaging; extract C(x) and Fourier spectrum S(k)\",\n                \"signature\": \"UV roll-off and oscillatory residuals consistent with discrete spectrum / modified dispersion\",\n                \"systematics\": [\"finite-size\", \"trap inhomogeneity\", \"detector resolution\", \"thermal background\"],\n            },\n            {\n                \"priority\": 2,\n                \"name\": \"Unequal-time correlator C(t) and retarded Green's function\",\n                \"platforms\": [\"optomechanical arrays\", \"cold-atom simulators\"],\n                \"observable\": \"C(t)=<phi(0)phi(t)>, G_R(t)\",\n                \"measurement\": \"pump-probe; quench then read out temporal correlations\",\n                \"signature\": \"phase slip / beating from discrete modes; dispersive group-delay shifts\",\n                \"systematics\": [\"dispersion calibration\", \"sampling jitter\", \"bath coupling\"],\n            },\n            {\n                \"priority\": 3,\n                \"name\": \"Entanglement diagnostics via mutual information / negativity proxy\",\n                \"platforms\": [\"continuous-variable photonics\", \"cQED\", \"ion chains\"],\n                \"observable\": \"I(A:B) from covariance matrix; logarithmic negativity if accessible\",\n                \"measurement\": \"Gaussian tomography / quadrature readout; build covariance blocks\",\n                \"signature\": \"scale-dependent suppression/enhancement across partitions tracking ell, rho\",\n                \"systematics\": [\"loss\", \"mode mismatch\", \"finite squeezing\", \"classical correlations\"],\n            },\n            {\n                \"priority\": 4,\n                \"name\": \"Astrophysical time-of-flight dispersion constraints\",\n                \"platforms\": [\"GRB/FRB timing\", \"GW multi-messenger\"],\n                \"observable\": \"energy-dependent arrival delays\",\n                \"measurement\": \"cross-band timing; hierarchical population model\",\n                \"signature\": \"Δt ∝ alpha*ell^2*E^2*D (toy), bounded by non-detection\",\n                \"systematics\": [\"intrinsic source lags\", \"plasma dispersion\", \"selection biases\"],\n            },\n        ],\n    }\ndef simulate(outdir: Path, n: int\n"
        }
      ]
    },
    {
      "id": "ci_0c8a19b036380f61006949b2b212f88197b4fcc7adfdefe897",
      "type": "code_interpreter_call",
      "status": "completed",
      "code": "print(cli_text[4000:8000])\n",
      "container_id": "cntr_6949aff88f24819097f165c497ce475e074f2320f5d510c4",
      "outputs": [
        {
          "type": "logs",
          "logs": ", dt: float, xi: float, omega: float, ell: float, alpha: float,\n             rho: float, L: float, noise: float, seed: int) -> dict:\n    random.seed(seed)\n    # Toy model: correlator with dispersive correction and finite-size mode discretization.\n    k0 = math.pi / max(L, 1e-9)  # IR mode spacing\n    omega_eff = omega * (1.0 + alpha * (ell * k0) ** 2)\n    times = [i * dt for i in range(n)]\n    C = []\n    for t in times:\n        base = math.exp(-t / max(xi, 1e-9)) * math.cos(omega_eff * t)\n        # Microstructure density proxy: small-amplitude ripple scaling with rho.\n        ripple = 0.02 * math.tanh(rho) * math.cos(3.0 * omega_eff * t)\n        y = base + ripple + random.gauss(0.0, noise)\n        C.append(y)\n    # Gaussian-state proxy: mutual information from an effective correlation coefficient r(t0).\n    t0 = times[min(n - 1, max(1, n // 10))]\n    r = max(-0.999, min(0.999, math.exp(-t0 / max(xi, 1e-9)) * (1.0 - 0.5 * alpha * (ell * k0) ** 2)))\n    mutual_info = float(-0.5 * math.log(1.0 - r * r))\n    return {\n        \"model\": \"toy_discrete_dispersion_v1\",\n        \"inputs\": {\"n\": n, \"dt\": dt, \"xi\": xi, \"omega\": omega, \"ell\": ell, \"alpha\": alpha, \"rho\": rho, \"L\": L, \"noise\": noise, \"seed\": seed},\n        \"data\": {\"t\": times, \"C\": C, \"mutual_info\": mutual_info},\n        \"summary\": {\"C0\": C[0], \"C_rms\": float(math.sqrt(sum(x*x for x in C)/len(C)))},\n    }\ndef infer(data_path: Path, outdir: Path, ell_min: float, ell_max: float, m: int, alpha: float, L: float) -> dict:\n    obj = json.loads(data_path.read_text(encoding=\"utf-8\"))\n    t = obj[\"data\"][\"t\"]; Cobs = obj[\"data\"][\"C\"]\n    xi = float(obj[\"inputs\"][\"xi\"]); omega = float(obj[\"inputs\"][\"omega\"]); dt = float(obj[\"inputs\"][\"dt\"])\n    noise = float(obj[\"inputs\"][\"noise\"])\n    k0 = math.pi / max(L, 1e-9)\n    sig2 = max(noise * noise, 1e-12)\n    grid = []\n    for j in range(m):\n        ell = ell_min + (ell_max - ell_min) * (j / max(1, m - 1))\n        omega_eff = omega * (1.0 + alpha * (ell * k0) ** 2)\n        chi2 = 0.0\n        for ti, yi in zip(t, Cobs):\n            mu = math.exp(-ti / max(xi, 1e-9)) * math.cos(omega_eff * ti)\n            chi2 += (yi - mu) ** 2 / sig2\n        logL = -0.5 * chi2\n        grid.append({\"ell\": ell, \"logL\": float(logL)})\n    # Normalize to a discrete posterior with flat prior over ell.\n    mx = max(g[\"logL\"] for g in grid)\n    w = [math.exp(g[\"logL\"] - mx) for g in grid]\n    Z = sum(w) or 1.0\n    for gi, wi in zip(grid, w):\n        gi[\"posterior\"] = float(wi / Z)\n    ell_map = max(grid, key=lambda g: g[\"posterior\"])[\"ell\"]\n    mean = sum(g[\"ell\"] * g[\"posterior\"] for g in grid)\n    cdf = []\n    s = 0.0\n    for g in sorted(grid, key=lambda z: z[\"ell\"]):\n        s += g[\"posterior\"]; cdf.append((g[\"ell\"], s))\n    def q(p: float) -> float:\n        for e, cp in cdf:\n            if cp >= p: return float(e)\n        return float(cdf[-1][0])\n    return {\"data_ref\": str(data_path), \"alpha\": alpha, \"L\": L, \"ell_grid\": grid,\n            \"point_estimates\": {\"ell_map\": float(ell_map), \"ell_mean\": float(mean), \"ell_q16\": q(0.16), \"ell_q84\": q(0.84)}}\ndef build_parser() -> argparse.ArgumentParser:\n    p = argparse.ArgumentParser(prog=\"dgpipe\", description=\"Discrete-gravity pipeline CLI: protocols, simulation, inference.\")\n    p.add_argument(\"--outdir\", type=Path, default=Path(\"dgpipe_out\"), help=\"Output directory (created if missing).\")\n    sub = p.add_subparsers(dest=\"cmd\", required=True)\n\n    sp = sub.add_parser(\"protocols\", help=\"Write a prioritized measurement-protocol specification (JSON).\")\n\n    ss = sub.add_parser(\"simulate\", help=\"Run a controlled toy simulation with finite-size and dispersive systematics.\")\n    ss.add_argument(\"--n\", type=int, default=512); ss.add_argument(\"--dt\", type=float, default=0.01)\n    ss.add_argument(\"--xi\", type=float, default=1.0); ss.add_argument(\"--omega\", type=float, default=6.0)\n    ss.add_argument(\"--ell\", type=float, default=0.1); ss.add_argument(\"--alpha\", type=float, default=1.0)\n    ss.add_argument(\"--rho\", type=float, d\n"
        }
      ]
    },
    {
      "id": "ci_0c8a19b036380f61006949b2b40b4881979da572042adcbf59",
      "type": "code_interpreter_call",
      "status": "completed",
      "code": "print(cli_text[8000:12000])\n",
      "container_id": "cntr_6949aff88f24819097f165c497ce475e074f2320f5d510c4",
      "outputs": [
        {
          "type": "logs",
          "logs": "efault=1.0); ss.add_argument(\"--L\", type=float, default=10.0)\n    ss.add_argument(\"--noise\", type=float, default=0.02); ss.add_argument(\"--seed\", type=int, default=0)\n\n    si = sub.add_parser(\"infer\", help=\"Infer discreteness scale ell from a saved simulation/measurement JSON.\")\n    si.add_argument(\"data\", type=Path, help=\"Path to JSON produced by simulate (or compatible measurement file).\")\n    si.add_argument(\"--ell-min\", type=float, default=0.0); si.add_argument(\"--ell-max\", type=float, default=0.5)\n    si.add_argument(\"--m\", type=int, default=101); si.add_argument(\"--alpha\", type=float, default=1.0)\n    si.add_argument(\"--L\", type=float, default=10.0)\n    return p\n\ndef main(argv: list[str] | None = None) -> int:\n    argv = list(sys.argv[1:] if argv is None else argv)\n    args = build_parser().parse_args(argv)\n    outdir: Path = args.outdir\n    outdir.mkdir(parents=True, exist_ok=True)\n    prov = _provenance([\"dgpipe\"] + argv, outdir)\n\n    if args.cmd == \"protocols\":\n        payload = {\"provenance\": prov, \"protocol_spec\": generate_protocols()}\n        path = outdir / \"protocols.json\"\n        _write_json(path, payload)\n        print(str(path))\n        return 0\n\n    if args.cmd == \"simulate\":\n        sim = simulate(outdir, args.n, args.dt, args.xi, args.omega, args.ell, args.alpha, args.rho, args.L, args.noise, args.seed)\n        payload = {\"provenance\": prov, **sim}\n        path = outdir / \"simulation.json\"\n        _write_json(path, payload)\n        (outdir / \"simulation.sha256\").write_text(_sha256_bytes(path.read_bytes()) + \"\\n\", encoding=\"utf-8\")\n        print(str(path))\n        return 0\n\n    if args.cmd == \"infer\":\n        inf = infer(args.data, outdir, args.ell_min, args.ell_max, args.m, args.alpha, args.L)\n        payload = {\"provenance\": prov, **inf}\n        path = outdir / \"inference.json\"\n        _write_json(path, payload)\n        (outdir / \"inference.sha256\").write_text(_sha256_bytes(path.read_bytes()) + \"\\n\", encoding=\"utf-8\")\n        print(str(path))\n        return 0\n\n    raise SystemExit(\"Unknown command\")\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n\n"
        }
      ]
    },
    {
      "id": "rs_0c8a19b036380f61006949b2b5fea881978962cc0d0503c013",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "ci_0c8a19b036380f61006949b2bc8cc08197b1f9999837a96a53",
      "type": "code_interpreter_call",
      "status": "completed",
      "code": "prot_text=(pkg/\"protocols.py\").read_text()\nprint(prot_text[-1200:])\n",
      "container_id": "cntr_6949aff88f24819097f165c497ce475e074f2320f5d510c4",
      "outputs": [
        {
          "type": "logs",
          "logs": "         \"mapping\": \"ell_d sets high-frequency phase residual scale via modified dispersion\",\n        },\n        references=[\"parametrized GW tests\", \"Lorentz-violating dispersion\"],\n    ),\n]\ndef list_protocols(platform: Optional[str] = None, min_priority: int = 0) -> List[Dict[str, Any]]:\n    \"\"\"Return protocols as dicts, filtered and sorted by descending priority.\"\"\"\n    prots: Iterable[Protocol] = _PROTOCOLS\n    if platform:\n        p = platform.lower()\n        prots = [x for x in prots if p in x.platform.lower()]\n    prots = [x for x in prots if x.priority >= min_priority]\n    return [x.to_dict() for x in sorted(prots, key=lambda z: z.priority, reverse=True)]\n\n\ndef get_protocol(protocol_id: str) -> Dict[str, Any]:\n    \"\"\"Fetch a protocol by id; raises KeyError if missing.\"\"\"\n    for p in _PROTOCOLS:\n        if p.id == protocol_id:\n            return p.to_dict()\n    raise KeyError(f\"Unknown protocol id: {protocol_id}\")\n\n\ndef export_protocols_json() -> str:\n    \"\"\"Export the full registry as a JSON string (stable key ordering).\"\"\"\n    return json_dumps(list_protocols())\n\n\ndef json_dumps(obj: Any) -> str:\n    import json\n    return json.dumps(obj, indent=2, sort_keys=True) + \"\\n\"\n\n"
        }
      ]
    },
    {
      "id": "rs_0c8a19b036380f61006949b2bef9148197b0cb64871a864252",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "ci_0c8a19b036380f61006949b2c369908197a9c1cce8b10dd3a2",
      "type": "code_interpreter_call",
      "status": "completed",
      "code": "import numpy as np, json, hashlib\nfrom dgpipe.simulations import simulate_equal_time_correlator, SimConfig, DiscreteParams, NoiseModel\nx=np.linspace(0,5,11)\ncfg=SimConfig(L=64.0,N_modes=256,k_cut=None)\ndp=DiscreteParams(a=1.0,alpha=0.2,power=2,m=0.1)\nnoise=NoiseModel(sigma=1e-3,seed=123)\nout=simulate_equal_time_correlator(x,cfg=cfg,dp=dp,noise=noise)\nout[\"y\"][:3], out[\"yerr\"][:3]\n",
      "container_id": "cntr_6949aff88f24819097f165c497ce475e074f2320f5d510c4",
      "outputs": [
        {
          "type": "logs",
          "logs": "([0.5728735058130552, 0.5231168896221478, 0.4181115604963545],\n [0.001, 0.001, 0.001])"
        }
      ]
    },
    {
      "id": "rs_0c8a19b036380f61006949b2c78c4c8197924044cdc4eb1e60",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "ci_0c8a19b036380f61006949b2d4bae0819780e3da7e3adec9ef",
      "type": "code_interpreter_call",
      "status": "completed",
      "code": "from dgpipe.inference import InferenceProblem, map_fit_random\nInferenceProblem\n",
      "container_id": "cntr_6949aff88f24819097f165c497ce475e074f2320f5d510c4",
      "outputs": [
        {
          "type": "logs",
          "logs": "dgpipe.inference.InferenceProblem"
        }
      ]
    },
    {
      "id": "rs_0c8a19b036380f61006949b2da42988197b017813adf06ffb1",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "ci_0c8a19b036380f61006949b2dbc56c819799563fd80b4f49e6",
      "type": "code_interpreter_call",
      "status": "completed",
      "code": "from pathlib import Path\nimport json\ntarget_path = Path('/mnt/data').joinpath('tests/test_pipeline_smoke.py')\ntarget_path.parent.mkdir(parents=True, exist_ok=True)\n\nchunks = [\n\"\"\"import json\nimport numpy as np\nimport pytest\n\n\ndef test_protocol_registry_smoke():\n    from dgpipe.protocols import list_protocols, get_protocol\n\n    prots = list_protocols(min_priority=90)\n    assert len(prots) >= 2\n    assert prots[0][\"priority\"] >= prots[1][\"priority\"]\n    p0 = get_protocol(prots[0][\"id\"])\n    assert p0[\"schema_version\"] == \"1.0\"\n    assert isinstance(p0[\"protocol_steps\"], list)\n    assert \"params\" in p0 and isinstance(p0[\"params\"], dict)\n\"\"\",\n\"\"\"def test_simulation_deterministic_snapshot():\n    from dgpipe.simulations import DiscreteParams, NoiseModel, SimConfig, simulate_equal_time_correlator\n\n    x = np.linspace(0.0, 5.0, 11)\n    cfg = SimConfig(L=64.0, N_modes=256, k_cut=None)\n    dp = DiscreteParams(a=1.0, alpha=0.2, power=2, m=0.1)\n    noise = NoiseModel(sigma=1e-3, seed=123)\n\n    out = simulate_equal_time_correlator(x, cfg=cfg, dp=dp, noise=noise)\n    assert out[\"protocol\"] == \"equal_time_correlator\"\n    assert out[\"meta\"][\"discrete\"][\"alpha\"] == 0.2\n    np.testing.assert_allclose(out[\"y\"][:3], [0.5728735058130552, 0.5231168896221478, 0.4181115604963545], rtol=0, atol=1e-12)\n    assert all(float(s) == pytest.approx(1e-3) for s in out[\"yerr\"])\n\"\"\",\n\"\"\"def test_inference_recovers_alpha_map():\n    from dgpipe.inference import InferenceProblem, map_fit_random\n    import dgpipe.simulations as sim\n\n    x = np.linspace(0.0, 6.0, 25)\n    cfg = sim.SimConfig(L=64.0, N_modes=256)\n    dp_true = sim.DiscreteParams(a=1.0, alpha=0.15, power=2, m=0.05)\n    y = sim._wightman(0.0, x, cfg, dp_true)\n    yobs, yerr = sim._add_noise(y, sim.NoiseModel(sigma=2e-3, seed=7))\n\n    def model(theta, xx):\n        dp = sim.DiscreteParams(a=1.0, alpha=float(theta[0]), power=2, m=0.05)\n        return sim._wightman(0.0, np.asarray(xx, dtype=float), cfg, dp)\n\n    prob = InferenceProblem(x=x, y=yobs, sigma=yerr, model=model, param_names=(\"alpha\",), priors={\"alpha\": (\"uniform\", 0.0, 0.4)})\n    rng = np.random.default_rng(0)\n    best, _ = map_fit_random(prob.logpost, bounds=[(0.0, 0.4)], nsamples=3000, rng=rng)\n    assert float(best[0]) == pytest.approx(0.15, abs=0.03)\n\"\"\",\n\"\"\"def test_cli_end_to_end_smoke(tmp_path, capsys):\n    from dgpipe.cli import main\n\n    outdir = tmp_path / \"out\"\n    rc = main([\"--outdir\", str(outdir), \"protocols\"])\n    assert rc == 0 and (outdir / \"protocols.json\").exists()\n\n    rc = main(\n        [\n            \"--outdir\",\n            str(outdir),\n            \"simulate\",\n            \"--n\",\n            \"64\",\n            \"--dt\",\n            \"0.02\",\n            \"--xi\",\n            \"1.2\",\n            \"--omega\",\n            \"5.0\",\n            \"--ell\",\n            \"0.15\",\n            \"--alpha\",\n            \"1.0\",\n            \"--rho\",\n            \"0.8\",\n            \"--L\",\n            \"10.0\",\n            \"--noise\",\n            \"0.0\",\n            \"--seed\",\n            \"0\",\n        ]\n    )\n    assert rc == 0 and (outdir / \"simulation.json\").exists()\n\n    sim = json.loads((outdir / \"simulation.json\").read_text(encoding=\"utf-8\"))\n    assert sim[\"model\"] == \"toy_discrete_dispersion_v1\"\n    assert len(sim[\"data\"][\"t\"]) == 64 and len(sim[\"data\"][\"C\"]) == 64\n    assert np.isfinite(sim[\"data\"][\"mutual_info\"])\n\n    rc = main(\n        [\n            \"--outdir\",\n            str(outdir),\n            \"infer\",\n            str(outdir / \"simulation.json\"),\n            \"--ell-min\",\n            \"0.0\",\n            \"--ell-max\",\n            \"0.3\",\n            \"--m\",\n            \"61\",\n            \"--alpha\",\n            \"1.0\",\n            \"--L\",\n            \"10.0\",\n        ]\n    )\n    assert rc == 0 and (outdir / \"inference.json\").exists()\n    inf = json.loads((outdir / \"inference.json\").read_text(encoding=\"utf-8\"))\n    assert \"point_estimates\" in inf and \"ell_map\" in inf[\"point_estimates\"]\n    assert float(inf[\"point_estimates\"][\"ell_map\"]) == pytest.approx(0.15, abs=1e-9)\n\"\"\",\n]\n\nfinal_text = '\\n'.join(block.strip('\\n') for block in chunks).strip() + '\\n'\ntarget_path.write_text(final_text, encoding='utf-8')\nprint('FILE_WRITTEN:tests/test_pipeline_smoke.py')\nprint('DIR_STATE:' + json.dumps(sorted(str(p.relative_to(Path(\"/mnt/data\"))) for p in target_path.parent.glob('*') if p.is_file())))\n",
      "container_id": "cntr_6949aff88f24819097f165c497ce475e074f2320f5d510c4",
      "outputs": [
        {
          "type": "logs",
          "logs": "FILE_WRITTEN:tests/test_pipeline_smoke.py\nDIR_STATE:[\"tests/test_pipeline_smoke.py\"]\n"
        }
      ]
    },
    {
      "id": "msg_0c8a19b036380f61006949b2f51ed88197ac3e25cd9042bccb",
      "type": "message",
      "status": "completed",
      "content": [
        {
          "type": "output_text",
          "annotations": [],
          "logprobs": [],
          "text": "Created `tests/test_pipeline_smoke.py` with deterministic end-to-end pytest smoke coverage for the protocol registry, simulation snapshot reproducibility, MAP-style parameter recovery via inference, and a full CLI run (protocols → simulate → infer) with validated JSON outputs."
        }
      ],
      "role": "assistant"
    }
  ],
  "codeResults": []
}