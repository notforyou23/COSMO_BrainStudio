You are inside the OpenAI code interpreter environment with filesystem access to /mnt/data.

Mission summary: Add `pytest` tests and GitHub Actions to (i) validate all example inputs against schema, (ii) run the reference implementation on `benchmark_case_001`, (iii) compare produced outputs to expected within tolerance, and (iv) upload outputs as CI artifacts for inspection.
Project: generated_script_1766433269260 (python script)

Target file details:
- Path: tools/ci_run_benchmark.py
- Purpose: Standalone runner script invoked by CI to execute the reference implementation on benchmark_case_001 and persist produced outputs to an artifacts directory for upload and inspection.
- Category: support

Other planned files (for context only):
- .github/workflows/ci.yml: GitHub Actions workflow that installs dependencies, runs pytest, executes the benchmark_case_001 reference run, compares outputs to expected within tolerance, and uploads produced outputs as CI artifacts.
- tests/test_schema_conformance.py: Pytest suite that validates all example JSON inputs under the examples directory against the benchmark input JSON Schema using jsonschema.
- tests/test_benchmark_case_001.py: Pytest suite that runs the reference implementation on benchmark_case_001, writes outputs to a deterministic location, and compares them to expected outputs within numeric tolerances.
- tests/conftest.py: Pytest fixtures for locating repository paths, loading JSON examples/expected files, configuring tolerances, and defining the output directory used by CI and local runs.

Reference insights:
- [INTROSPECTION] 2025-12-22T18-56-46-314Z_outputs_schemas_benchmark_schema_json_stage1_attempt1_prompt.txt from code-creation agent agent_176
- [INTROSPECTION] 2025-12-22T19-27-20-465Z_requirements-ci_txt_stage1_export_export_prompt.txt from code-creation agent agent_1766431635741_ky
- [INTROSPECTION] 2025-12-22T19-06-56-881Z_src_lib_plotting_py_stage1_export_export_prompt.txt from code-creation agent agent_1766430411939_w8
- [AGENT: agent_1766429554962_lz72do0] Foundations work (2019â€“2025) increasingly prioritizes operational/testable frameworks (causal modeling,
- [INTROSPECTION] 2025-12-22T18-56-46-314Z_outputs_expected_benchmark_case_001_expected_json_stage1_attempt1_prompt.txt from code-creation age

Stage details:
- Stage: Stage 1
- Mode: create
- Goal: Standalone runner script invoked by CI to execute the reference implementation on benchmark_case_001 and persist produced outputs to an artifacts directory for upload and inspection.
- Line budget for this stage: <= 150 new/changed lines.

Implementation tasks (execute using Python in this environment):
1. from pathlib import Path
2. import json
3. target_path = Path('/mnt/data').joinpath('tools/ci_run_benchmark.py')
4. target_path.parent.mkdir(parents=True, exist_ok=True)
5. Build the entire stage deliverable (<= 150 lines) as a list named chunks where each item is a multiline string representing a contiguous block of code without leading or trailing blank lines.
6. final_text = '\n'.join(block.strip('\n') for block in chunks).strip() + '\n'
7. target_path.write_text(final_text, encoding='utf-8')
8. print('FILE_WRITTEN:tools/ci_run_benchmark.py')
9. print('DIR_STATE:' + json.dumps(sorted(str(p.relative_to(Path("/mnt/data"))) for p in target_path.parent.glob('*') if p.is_file())))

Constraints:
- Ensure the new file fully satisfies the stage goal with no placeholders or TODO markers.
- Keep the newly written code within the stated line/character budget.
- Keep console output short (only FILE_WRITTEN / DIR_STATE plus essential status).
- Do not touch files outside tools/ci_run_benchmark.py.
- After writing the file, provide a one-sentence summary. Do not list the file contents in the final message.
