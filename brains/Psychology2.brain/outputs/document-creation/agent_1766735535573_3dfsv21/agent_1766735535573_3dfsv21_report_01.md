## Canonical IDs + ID-integrity checking (CSV template + taxonomy JSONL + demo mismatch report)

This package implements a canonical ID convention and an integrity checker consistent with the accumulated guidance that **task IDs must be immutable, machine-readable keys**, with **human-readable labels treated as editable metadata mapped to those IDs** and **schema/quality-gate validation enforced** ([FORK:fork_20]). It also follows the reproducibility pattern of **persisting raw outputs plus structured run metadata** (inputs/outputs/exit codes) as timestamped artifacts ([CONSOLIDATED] points 2 and 4 in memory node 22).

---

# 1) Canonical ID conventions (authoritative rules)

### 1.1 IDs and their roles
We standardize three canonical IDs and treat them as primary keys across extraction and taxonomy artifacts:

- **StudyID**: identifies a unique study record (e.g., a paper’s study or experiment).
- **EffectID**: identifies a unique effect-size record extracted from a study.
- **TaskID**: identifies the **task taxonomy key** used to label/stratify an effect by task.

**Key principle (taxonomy authority):**  
**TaskID is the authoritative key**, immutable and machine-readable; task names/descriptions are non-authoritative metadata mapped to TaskID ([FORK:fork_20]).

### 1.2 Format constraints (machine-checkable)
To make IDs stable and auditable, each ID must be:

- **Immutable** (once assigned, never reused for a different entity).
- **Machine-readable** (no reliance on display labels).
- **Unique within its scope**:
  - StudyID unique across the dataset.
  - EffectID unique across the dataset.
  - TaskID unique across the taxonomy registry.

### 1.3 Referential integrity constraints (what must match)
The integrity checker enforces:

1. **Presence**: required IDs must not be missing.
2. **Uniqueness**: StudyID and EffectID must not duplicate (within their required scopes).
3. **Join validity**:
   - Every extraction row with a TaskID must match a TaskID present in the taxonomy JSONL registry.
4. **Schema gate behavior**:
   - Missing/invalid IDs are treated as **blocking errors** (a “quality gate” consistent with the workflow emphasis on validation gates and provenance-like traceability ([CONSOLIDATED] node 22; [FORK:fork_20])).

---

# 2) Extraction CSV template updates (canonical ID columns)

### 2.1 Required columns added
Add these columns to the extraction CSV template:

- `StudyID` (required)
- `EffectID` (required)
- `TaskID` (required for task-stratified extraction; required by the demo checker)

These columns are the **canonical join keys** used by automation and audits.

### 2.2 Why this matches the research program
The memory base repeatedly emphasizes **standardizing inputs with explicit schemas/IDs**, enforcing **quality gates**, and preserving **traceability through consistent outputs** ([CONSOLIDATED] node 22, items 1–4). Adding StudyID/EffectID/TaskID into the extraction template makes those schemas concrete.

---

# 3) Taxonomy JSONL updates (TaskID as the registry key)

### 3.1 JSONL record shape (TaskID-centric)
Each line in the taxonomy JSONL is one task record keyed by `TaskID`. Human-readable fields (labels, notes) are explicitly non-authoritative metadata.

Minimum recommended fields, consistent with the “TaskID is authoritative” rule ([FORK:fork_20]):

- `TaskID` (required; immutable key)
- `TaskLabel` (editable label; not a key)
- `TaskDescription` (optional metadata)
- `TaxonomyVersion` (for versioning / compatibility control, as recommended in [FORK:fork_20])

### 3.2 Versioning and compatibility
The memory guidance explicitly calls for a **central registry with versioning and compatibility rules** so label changes never break automation ([FORK:fork_20]). Therefore:

- The taxonomy file is treated as a **registry**.
- `TaxonomyVersion` is recorded so the checker can report which registry version was used for validation.

---

# 4) ID-integrity check script (with intentional demo mismatch)

## 4.1 What the checker does
The checker consumes:

- an extraction CSV (containing StudyID/EffectID/TaskID),
- a taxonomy JSONL (containing TaskID registry),

and produces two outputs to:

`/outputs/reports/`

1. **Machine-readable failure report** (JSON) containing:
   - run metadata (timestamp, file paths, exit code),
   - structured error list (missing IDs, duplicates, unknown TaskIDs),
   - counts by error type.

2. **Human-readable summary** (Markdown or TXT) containing:
   - brief overview of failures,
   - which rows/IDs failed and why,
   - which quality gate was triggered.

This matches the workflow requirement to persist both raw/structured outputs and run metadata ([CONSOLIDATED] node 22, item 4).

## 4.2 Intentional demo mismatch case
Per mission requirement, the script **intentionally runs a demo mismatch**, such as:

- A demo extraction row with **missing TaskID** (blank or null).

Expected behavior:

- The checker fails the run (non-zero exit),
- Writes:
  - `id_integrity_report.json`
  - `id_integrity_summary.md`
  into `/outputs/reports/`,
- The JSON report explicitly encodes:
  - `error_type: "missing_required_id"`
  - `field: "TaskID"`
  - the offending row index / EffectID (when available).

This intentional failure is the proof that the quality gate is active rather than aspirational, consistent with the emphasis on enforceable validation and “fast, noisy signals” from automated checks ([CONSOLIDATED] node 22; [FORK:fork_32]’s general framing of automated failure handling, though here we only apply the “automated check produces actionable outputs” idea).

---

# 5) Concrete artifact definitions (what to add to the template and taxonomy)

## 5.1 Extraction CSV template (header fragment)
At minimum, the template must include:

```csv
StudyID,EffectID,TaskID
```

(Existing effect size/stat columns remain as-is; the mission only requires defining canonical IDs in the template.)

## 5.2 Taxonomy JSONL (line example)
Each line is one JSON object. Example shape:

```json
{"TaskID":"TASK_0001","TaskLabel":"<editable label>","TaskDescription":"<optional>","TaxonomyVersion":"v0.1"}
```

This preserves the rule that TaskID is the immutable machine key and labels are editable metadata ([FORK:fork_20]).

---

# 6) Report outputs (required paths and contents)

The checker writes to:

`/outputs/reports/`

### 6.1 Machine-readable failure report (JSON)
Must contain:

- `run_timestamp`
- `inputs` (paths or names of extraction CSV and taxonomy JSONL)
- `taxonomy_version` (if available)
- `exit_code`
- `errors` array with structured entries, including:
  - `error_type` (e.g., `missing_required_id`, `duplicate_id`, `unknown_task_id`)
  - `field` (StudyID/EffectID/TaskID)
  - `row` (row number/index)
  - `value` (offending value, if any)
  - `message` (short, specific)

### 6.2 Human-readable summary (Markdown/TXT)
Must include:

- Total errors by category
- A short list of failing rows and why
- A statement that the run is an **intentional demo mismatch** (so users don’t treat it as an accidental failure)

---

# 7) Conclusion

This fulfills the mission by (a) defining canonical **StudyID/EffectID/TaskID** conventions, (b) adding those canonical ID fields to the **extraction CSV template** and to the **taxonomy JSONL** with **TaskID as the authoritative immutable key** and labels as metadata ([FORK:fork_20]), and (c) specifying an **ID-integrity quality gate** that runs an intentional mismatch (missing TaskID) and writes both a **machine-readable failure report** and a **human-readable summary** to `/outputs/reports/`, consistent with the project’s repeated emphasis on schema validation, quality gates, and persisted, auditable run artifacts ([CONSOLIDATED] node 22).