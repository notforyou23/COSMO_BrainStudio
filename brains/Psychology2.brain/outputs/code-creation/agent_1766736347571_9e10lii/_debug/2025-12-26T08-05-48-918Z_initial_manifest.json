{
  "manifestVersion": "2.0.0-plan",
  "agentId": "agent_1766736347571_9e10lii",
  "agentType": "code-creation",
  "goalId": "goal_98",
  "missionId": "strategic_goal_98_1766736347571",
  "spawnCycle": 126,
  "coordinatorReview": null,
  "spawnedBy": "strategic_spawner",
  "triggerSource": "strategic_goal",
  "spawningReason": "high",
  "createdAt": "2025-12-26T08:05:58.876Z",
  "completedAt": null,
  "canonical": false,
  "supersedes": [],
  "supersededBy": null,
  "deliverableType": "code",
  "files": [
    {
      "path": "tests/fixtures/doi_fixture_list.json",
      "description": "Defines a small, fixed list of DOI test fixtures with metadata (expected success/failure and notes) used by the integration test.",
      "category": "test",
      "stage": 1,
      "stageLabel": "Stage 1",
      "stageGoal": null,
      "mode": "create",
      "maxLines": 150,
      "maxChars": null,
      "status": "pending",
      "attempts": 0,
      "errors": [],
      "createdAt": "2025-12-26T08:05:58.876Z"
    },
    {
      "path": "tests/integration/test_doi_fixture_integration.py",
      "description": "Runs api_server.py (or the CLI) against the fixed DOI fixture list, writes JSON/CSV outputs to runtime/_build or outputs/tools, and asserts expected success/failure behavior per fixture.",
      "category": "test",
      "stage": 1,
      "stageLabel": "Stage 1",
      "stageGoal": null,
      "mode": "create",
      "maxLines": 150,
      "maxChars": null,
      "status": "pending",
      "attempts": 0,
      "errors": [],
      "createdAt": "2025-12-26T08:05:58.876Z"
    },
    {
      "path": "tests/support/run_api_server_fixture.py",
      "description": "Provides a reusable subprocess-based runner that starts api_server.py (or calls the CLI), feeds the DOI fixtures, collects results, and persists artifacts deterministically for CI.",
      "category": "support",
      "stage": 1,
      "stageLabel": "Stage 1",
      "stageGoal": null,
      "mode": "create",
      "maxLines": 150,
      "maxChars": null,
      "status": "pending",
      "attempts": 0,
      "errors": [],
      "createdAt": "2025-12-26T08:05:58.876Z"
    },
    {
      "path": "tests/support/artifact_paths.py",
      "description": "Centralizes output directory selection and naming (preferring outputs/tools else runtime/_build) to keep artifact paths consistent across local runs and CI.",
      "category": "support",
      "stage": 1,
      "stageLabel": "Stage 1",
      "stageGoal": null,
      "mode": "create",
      "maxLines": 150,
      "maxChars": null,
      "status": "pending",
      "attempts": 0,
      "errors": [],
      "createdAt": "2025-12-26T08:05:58.876Z"
    },
    {
      "path": "docs/integration_doi_fixture_test.md",
      "description": "Documents how to run the DOI fixture integration test, where artifacts are saved, and the expected pass/fail cases with rationale and stabilization criteria for CI gating.",
      "category": "documentation",
      "stage": 1,
      "stageLabel": "Stage 1",
      "stageGoal": null,
      "mode": "create",
      "maxLines": 150,
      "maxChars": null,
      "status": "pending",
      "attempts": 0,
      "errors": [],
      "createdAt": "2025-12-26T08:05:58.876Z"
    },
    {
      "path": ".github/workflows/doi-fixture-integration.yml",
      "description": "Adds a CI workflow that executes the DOI fixture integration test and uploads the generated JSON/CSV artifacts, initially as non-blocking until marked stable.",
      "category": "config",
      "stage": 1,
      "stageLabel": "Stage 1",
      "stageGoal": null,
      "mode": "create",
      "maxLines": 150,
      "maxChars": null,
      "status": "pending",
      "attempts": 0,
      "errors": [],
      "createdAt": "2025-12-26T08:05:58.876Z"
    },
    {
      "path": "pytest.ini",
      "description": "Configures pytest markers and default options for separating integration tests (e.g., -m integration) and ensuring predictable test discovery in CI.",
      "category": "config",
      "stage": 1,
      "stageLabel": "Stage 1",
      "stageGoal": null,
      "mode": "create",
      "maxLines": 150,
      "maxChars": null,
      "status": "pending",
      "attempts": 0,
      "errors": [],
      "createdAt": "2025-12-26T08:05:58.876Z"
    }
  ],
  "integrationStatus": "pending",
  "reviewNotes": null,
  "missionDescription": "Create a minimal integration test that runs api_server.py (or the CLI) on a fixed DOI fixture list, saves results to /outputs/tools or runtime/_build (JSON/CSV), and documents expected success/failure cases; wire the run into CI once stable.",
  "missionSuccessCriteria": [
    "Complete strategic task identified by Meta-Coordinator or escalated by Strategic Tracker",
    "Curated insight (actionability 8-9/10): Adds execution validation for the citation MVP by running an end-to-end DOI list and saving results, turning the tool from a spec into a tested, auditable workflow and reducing regressions...."
  ],
  "projectName": "minimal integration test that runs api_server.py (or the CLI) on a fixed DOI fixture list, saves results to /outputs/tools or runtime/_build (JSON/CSV), and documents expected success/failure cases; wire the run into CI once stable.",
  "language": "python",
  "type": "api_server",
  "status": "in_progress",
  "requirements": [],
  "lastSavedAt": "2025-12-26T08:05:58.876Z"
}