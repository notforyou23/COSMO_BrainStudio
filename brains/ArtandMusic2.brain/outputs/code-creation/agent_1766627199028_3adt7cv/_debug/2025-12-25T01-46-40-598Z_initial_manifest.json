{
  "manifestVersion": "2.0.0-plan",
  "agentId": "agent_1766627199028_3adt7cv",
  "agentType": "code-creation",
  "goalId": "goal_225",
  "missionId": "strategic_goal_225_1766627199027",
  "spawnCycle": 136,
  "coordinatorReview": null,
  "spawnedBy": "strategic_spawner",
  "triggerSource": "strategic_goal",
  "spawningReason": "high",
  "createdAt": "2025-12-25T01:46:49.315Z",
  "completedAt": null,
  "canonical": false,
  "supersedes": [],
  "supersededBy": null,
  "deliverableType": "code",
  "files": [
    {
      "path": "scripts/verifier_benchmark.py",
      "description": "Main runnable benchmark harness implementing 2–3 verifier patterns, loading a grounded QA dataset with cited evidence, executing generate→verify→revise pipelines, and writing standardized result artifacts.",
      "category": "source",
      "stage": 1,
      "stageLabel": "Stage 1",
      "stageGoal": null,
      "mode": "create",
      "maxLines": 150,
      "maxChars": null,
      "status": "pending",
      "attempts": 0,
      "errors": [],
      "createdAt": "2025-12-25T01:46:49.315Z"
    },
    {
      "path": "src/verifiers/patterns.py",
      "description": "Implements verifier patterns (best-of-N + verifier rerank, entailment/attribution check over retrieved evidence, and self-consistency + critique gating) with a unified interface.",
      "category": "source",
      "stage": 1,
      "stageLabel": "Stage 1",
      "stageGoal": null,
      "mode": "create",
      "maxLines": 150,
      "maxChars": null,
      "status": "pending",
      "attempts": 0,
      "errors": [],
      "createdAt": "2025-12-25T01:46:49.315Z"
    },
    {
      "path": "src/verifiers/llm_backends.py",
      "description": "Provides pluggable model backends (OpenAI-compatible HTTP, local stub) for generation and verification prompts with retries, timeouts, and deterministic seeding.",
      "category": "source",
      "stage": 1,
      "stageLabel": "Stage 1",
      "stageGoal": null,
      "mode": "create",
      "maxLines": 150,
      "maxChars": null,
      "status": "pending",
      "attempts": 0,
      "errors": [],
      "createdAt": "2025-12-25T01:46:49.315Z"
    },
    {
      "path": "src/data/grounded_dataset.py",
      "description": "Loads and validates a grounded dataset schema (questions, answers, evidence snippets with citations) and exposes iteration plus canonical ID/evidence indexing.",
      "category": "source",
      "stage": 1,
      "stageLabel": "Stage 1",
      "stageGoal": null,
      "mode": "create",
      "maxLines": 150,
      "maxChars": null,
      "status": "pending",
      "attempts": 0,
      "errors": [],
      "createdAt": "2025-12-25T01:46:49.315Z"
    },
    {
      "path": "src/metrics/evaluation.py",
      "description": "Computes error-detection precision/recall/F1, AUROC/AUPRC, calibration curves (ECE/reliability diagram bins), and failure-mode slices from run outputs.",
      "category": "source",
      "stage": 1,
      "stageLabel": "Stage 1",
      "stageGoal": null,
      "mode": "create",
      "maxLines": 150,
      "maxChars": null,
      "status": "pending",
      "attempts": 0,
      "errors": [],
      "createdAt": "2025-12-25T01:46:49.315Z"
    },
    {
      "path": "src/metrics/plots.py",
      "description": "Generates calibration and diagnostic plots (reliability curves, PR/ROC, failure mode bar charts) from aggregated metrics and saves as PNG/SVG.",
      "category": "source",
      "stage": 1,
      "stageLabel": "Stage 1",
      "stageGoal": null,
      "mode": "create",
      "maxLines": 150,
      "maxChars": null,
      "status": "pending",
      "attempts": 0,
      "errors": [],
      "createdAt": "2025-12-25T01:46:49.315Z"
    },
    {
      "path": "src/utils/io.py",
      "description": "Handles artifact I/O including JSONL read/write, run directory management, and schema-safe serialization for prompts, responses, scores, and citations.",
      "category": "support",
      "stage": 1,
      "stageLabel": "Stage 1",
      "stageGoal": null,
      "mode": "create",
      "maxLines": 150,
      "maxChars": null,
      "status": "pending",
      "attempts": 0,
      "errors": [],
      "createdAt": "2025-12-25T01:46:49.315Z"
    },
    {
      "path": "configs/verifier_benchmark.yaml",
      "description": "Default configuration specifying dataset path, verifier patterns to run, model endpoints, sampling/best-of-N parameters, and metric/plot settings.",
      "category": "config",
      "stage": 1,
      "stageLabel": "Stage 1",
      "stageGoal": null,
      "mode": "create",
      "maxLines": 150,
      "maxChars": null,
      "status": "pending",
      "attempts": 0,
      "errors": [],
      "createdAt": "2025-12-25T01:46:49.315Z"
    },
    {
      "path": "data/sample_grounded_dataset.jsonl",
      "description": "Small grounded QA dataset sample in JSONL with cited evidence fields for quick local validation of the harness and metrics.",
      "category": "support",
      "stage": 1,
      "stageLabel": "Stage 1",
      "stageGoal": null,
      "mode": "create",
      "maxLines": 150,
      "maxChars": null,
      "status": "pending",
      "attempts": 0,
      "errors": [],
      "createdAt": "2025-12-25T01:46:49.315Z"
    },
    {
      "path": "README.md",
      "description": "Project documentation describing verifier patterns, dataset schema, how to run the benchmark, produced artifacts, and interpretation of metrics/calibration plots.",
      "category": "documentation",
      "stage": 1,
      "stageLabel": "Stage 1",
      "stageGoal": null,
      "mode": "create",
      "maxLines": 150,
      "maxChars": null,
      "status": "pending",
      "attempts": 0,
      "errors": [],
      "createdAt": "2025-12-25T01:46:49.315Z"
    }
  ],
  "integrationStatus": "pending",
  "reviewNotes": null,
  "missionDescription": "Select 2–3 verifier patterns to implement in a benchmark harness (e.g., best-of-N + verifier; entailment-check over retrieved evidence), define metrics (precision/recall for error detection, calibration curves), and run on a grounded dataset with cited evidence to compare failure modes.",
  "missionSuccessCriteria": [
    "Complete strategic task identified by Meta-Coordinator or escalated by Strategic Tracker",
    "Curated insight (actionability 8-9/10): The ‘generate → verify → revise’ framing and patterns (self-consistency, best-of-N + verifier, entailment-based RAG verification) directly inform the verifier architecture benchmark plan and helps dis..."
  ],
  "projectName": "generated_script_1766627199289",
  "language": "python",
  "type": "script",
  "status": "in_progress",
  "requirements": [],
  "lastSavedAt": "2025-12-25T01:46:49.315Z"
}