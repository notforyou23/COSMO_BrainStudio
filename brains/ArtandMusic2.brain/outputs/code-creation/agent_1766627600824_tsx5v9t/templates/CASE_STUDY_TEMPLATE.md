# Case Study: <Project / Decision / Intervention Name>

<!-- CASE_STUDY_TEMPLATE v1 -->
<!-- REQUIRED_MARKERS: CS_TITLE, CS_EXEC_SUMMARY, CS_CONTEXT, CS_PROBLEM, CS_APPROACH, CS_EVIDENCE, CS_FINDINGS, CS_RECOMMENDATIONS, CS_RISKS, CS_EQUITY, CS_IMPLEMENTATION, CS_METRICS, CS_LIMITATIONS, CS_APPENDIX -->

**Owner:**  
**Date:**  
**Stage:** Discovery / Pilot / Production  
**Audience:** Exec / Product / Policy / Research  
**Confidentiality:** Public / Internal / Restricted
## Executive Summary <!-- CS_EXEC_SUMMARY -->

**Decision to make:**  
**What we did (1–2 sentences):**  
**What we found (3–5 bullets):**
- 
- 
- 

**What we recommend (3–5 bullets):**
- 
- 
- 

**Expected impact (who benefits, how much, by when):**
## Context <!-- CS_CONTEXT -->

### Background
Describe the setting, why this matters now, and what constraints shaped the work (time, policy, capacity, budget, risk tolerance).

### Stakeholders
List key stakeholders and their incentives, responsibilities, and decision rights.

### Prior work
Summarize relevant prior analyses, experiments, incidents, or policy decisions that informed this case study.
## Problem Statement <!-- CS_PROBLEM -->

### The user / customer / community need
Define the need in plain language, including who experiences it and in what situations.

### Scope
**In scope:**  
**Out of scope:**  

### Success criteria
Define observable outcomes that would count as success (not just outputs). Include primary and secondary outcomes.
## Approach <!-- CS_APPROACH -->

### Hypotheses
State testable hypotheses and the expected direction of change.

### Design
Describe the evaluation or research design (e.g., experiment, quasi-experiment, cohort comparison, qualitative study, simulation). Specify the unit of analysis and time horizon.

### Data & sources
For each data source: owner, coverage period, key fields, known quality issues, and access constraints.

### Methods
Explain analyses performed (e.g., descriptive stats, regression, causal inference strategy, thematic coding). Include rationale and any preregistered choices if applicable.
## Evidence & Quality Checks <!-- CS_EVIDENCE -->

### Data quality
Document checks performed (missingness, duplicates, leakage, label validity, drift, sensitivity to exclusions). Summarize results.

### Reproducibility
Provide how to reproduce the analysis (repo path, command, environment notes). If not reproducible, state why and what would be required.

### Validation
Describe how results were validated (backtesting, holdout sets, peer review, stakeholder review, triangulation with qualitative input).
## Findings <!-- CS_FINDINGS -->

### Key results
Present the core results with enough detail to support decisions (effect sizes, confidence/uncertainty, practical significance).

### Heterogeneity
Report meaningful subgroup differences (by context, segment, geography, time) and whether they were expected.

### What surprised us
Call out counterintuitive results, null results, or results that changed the direction of the work.
## Recommendations <!-- CS_RECOMMENDATIONS -->

### Recommended decision
State the decision clearly (ship / do not ship / pilot / pause / revise). Include the rationale and tradeoffs.

### Alternatives considered
List alternatives, why they were considered, and why they were not chosen.

### Open questions
List unanswered questions that materially affect the decision and how to resolve them (data, experiments, stakeholder input).
## Risks & Mitigations <!-- CS_RISKS -->

### Operational risks
Failure modes in implementation, maintenance, adoption, incentives, monitoring, and escalation paths.

### Technical risks
Model/logic brittleness, drift, integration points, security/privacy risks, and dependency risks.

### Mitigation plan
Concrete mitigations with owners and timeframes (monitoring, guardrails, rollbacks, alerts, QA gates).
## Equity, Ethics, and Safety Considerations <!-- CS_EQUITY -->

### Who could be harmed or left behind
Identify affected groups, plausible harms, and power dynamics (including measurement choices as potential equity levers).

### Fairness / bias assessment
Document what was checked (representation, error rates, threshold impacts, accessibility) and what was not checked (and why).

### Safeguards
Human-in-the-loop steps, appeals, transparency, consent, and communication plan. Include how incidents will be handled.
## Implementation Plan <!-- CS_IMPLEMENTATION -->

### Rollout strategy
Phased rollout plan, eligibility, gating criteria, and rollback plan.

### Ownership & operating model
Who owns the system/process, SLAs, on-call/escalation, and governance.

### Dependencies
Data pipelines, vendors, policies, legal review, comms, training, and change management needs.
## Metrics & Monitoring <!-- CS_METRICS -->

### Primary metrics
Define metrics, formulas, expected direction, and acceptable ranges.

### Leading indicators
Early-warning signals and how frequently they are reviewed.

### Dashboards & alerts
Where metrics live, alert thresholds, and who is notified.
## Limitations <!-- CS_LIMITATIONS -->

Describe the main limitations (data coverage, measurement error, external validity, confounding, sample size, timing, implementation fidelity). Note how limitations could bias conclusions and what would reduce uncertainty.
## Appendix <!-- CS_APPENDIX -->

### Glossary
Define key terms and acronyms used in this case study.

### Sources & references
Cite documents, datasets, papers, policies, and stakeholder inputs that informed the work.

### Change log
Record meaningful changes to the case study over time (date, change, author).
